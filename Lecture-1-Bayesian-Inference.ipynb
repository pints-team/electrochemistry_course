{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Bayesian Inference and PINTS\n",
    "\n",
    "by Martin Robinson\n",
    "\n",
    "\n",
    "- Oxford Research Software Engineering group (http://www.cs.ox.ac.uk/projects/RSE)\n",
    "- Department of Computer Science, University of Oxford\n",
    "- Pints: https://github.com/pints-team/pints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Who am I?\n",
    "\n",
    "- PhD in Mathematical Science at Monash University\n",
    "- Now a:\n",
    "    - Senior Research Software Engineer in the [Oxford RSE group](http://www.cs.ox.ac.uk/projects/RSE)\n",
    "    - Co-director of EPSRC & MRC [Sustainable Approaches to Biomedical Science Centre for Doctoral Training: Responsible and Reproducible Research](https://sabsr3.web.ox.ac.uk), or SABS:R3 \n",
    "- Research Interests include:\n",
    "    - numerical modelling and simulation\n",
    "    - particle-based methods\n",
    "    - Bayesian inference\n",
    "    - developing robust and reliable software for research\n",
    "        - see [Aboria](https://github.com/aboria/Aboria), Chaste, PINTS, Smoldyn, SPH-DEM, PyBaMM and Trase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course Structure\n",
    "\n",
    "- Lecture 1: Introduction to Bayesian Inference and Pints\n",
    "   - What is Bayesian Inference?\n",
    "   - Bayes Theorem: Priors, Likelihood functions and Posteriors\n",
    "   - Introduction to Pints\n",
    "   - Using your own models in PINTS\n",
    "- Lecture 2: Maximum Likelihood Estimation\n",
    " \n",
    "- Lecture 3: MCMC sampling\n",
    "- Lecture 4: Hierarchical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture Structure\n",
    "\n",
    "- What is Bayesian Inference?\n",
    "- Bayes Theorem\n",
    "- Introdcution to Pints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Bayesian Statistics?\n",
    "\n",
    "- A statistical philosophy, or a way of thinking about probabilities\n",
    "\n",
    "**Frequentist:** $\\;P(A)\\;$ describes the limiting frequency of an event $A$. there is a fixed value of $\\;P(A)\\;$ that must be calculated e.g. proportion of heads from a fair coin toss will approach 0.5 after a large number of trials\n",
    "\n",
    "**Bayesian:** $\\;P(A)\\;$ is a measure of centainty, quantification of investigators belief that $\\;A\\;$ is true a fixed value of $\\;P(A)\\;$ is not neccessary, nor desirable. Pior information must be used to augment sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Bayesian Inference?\n",
    "\n",
    "Inference involves finding parameter values, or distributions of parameters values, for which model outputs are consistent with observations\n",
    "\n",
    "Bayesian inference uses Bayes' theorem to update prior beliefs after obtaining new data $y$\n",
    "\n",
    "- *Likelihood function:* the probability of obtaining the data $y$, given a set of parameters $\\theta$\n",
    "- *Prior probability distribution:* encodes your uncertainty in the parameters before the data $y$ has been obtained\n",
    "\n",
    "```\n",
    "                     Bayes' Theorem\n",
    "likelihood + prior -------------------> posterior\n",
    "```\n",
    "\n",
    "- *Posterior distribution:* updated probability distribution of $\\theta$, given the new data $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tangible benefits of Bayesian inference\n",
    "\n",
    "- Straightforward application to scientific modelling and experimentatal data analysis\n",
    "- Simple and intuitive model building (unlike frequentist statistics there is no need to remember lots of specific formulae).\n",
    "- Exhaustive and creative model testing.\n",
    "- Straightforward interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayes' Rule\n",
    "\n",
    "![Thomas Bayes](fig/225px-Thomas_Bayes.gif)\n",
    "\n",
    "$$P(\\theta | data) = \\frac{P(data|\\theta) P(\\theta)}{P(data)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Likelihoods\n",
    "\n",
    "$$P(\\theta | data) = \\frac{\\color{red}{{P(data|\\theta)}} P(\\theta)}{P(data)}$$\n",
    "\n",
    "- common to both Frequentist and Bayesian analyses\n",
    "- probability of generating the particular sample of data, given the model parameters $\\theta$\n",
    "- normally easy to obtain given a statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: flipping a coin\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "Take the classic example of tossing a fair coin that has a probability landing heads up of $\\theta = 0.5$\n",
    "\n",
    "If we flip the coin 2 times, we have the set of possible outcomes:\n",
    "\n",
    "$$P(H, H | \\theta = 0.5) = P(H|\\theta=0.5) P(H|\\theta=0.5) = 0.25$$\n",
    "$$P(H, T | \\theta = 0.5) = P(H|\\theta=0.5) P(T|\\theta=0.5) = 0.25$$\n",
    "$$P(T, H | \\theta = 0.5) = P(T|\\theta=0.5) P(H|\\theta=0.5) = 0.25$$\n",
    "$$P(T, T | \\theta = 0.5) = P(T|\\theta=0.5) P(T|\\theta=0.5) = 0.25$$\n",
    "\n",
    "This is a valid probability distribution:\n",
    "\n",
    "$$ P(H, H | \\theta = 0.5) + P(H, T | \\theta = 0.5) + P(T, H | \\theta = 0.5)+ P(T, T | \\theta = 0.5) = 1.0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating the likelihood\n",
    "\n",
    "- Hold the **data constant**, and find the likelihood of this data given a certain $\\theta$\n",
    "- Provides an infinite number of possibilities to arrive at the given data\n",
    "- Take for example the case of $\\text{data} = H, H$\n",
    "- The likelihood function is given by $P(data|\\theta) = \\theta^2$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.linspace(0,1,100)\n",
    "likelihood = theta*theta\n",
    "plt.plot(theta,likelihood)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('likelihood')\n",
    "plt.fill_between(theta,likelihood,alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is **not** a valid probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.trapz(likelihood,x=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Priors\n",
    "\n",
    "$$P(\\theta | data) = \\frac{{P(data|\\theta)} \\color{red}{P(\\theta)}}{P(data)}$$\n",
    "\n",
    "- This is particular to Bayesian inference, where we always update our *prior* beliefs using the new data\n",
    "- This is a function of all the possible parameter values $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Prior\n",
    "\n",
    "Back to the coin example, we know that the possible domain for our parameter is $\\theta \\in [0, 1]$.\n",
    "\n",
    "1. One option is the consider all the possible values of $\\theta$ to be equally likely (i.e. a Uniform prior).\n",
    "2. Another is to use our previous knowledge that most coins are likely to be fair (i.e. a Gaussian prior around $\\theta = 0.5$)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "mu = 0\n",
    "variance = 0.1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x,scipy.stats.norm.pdf(x, mu, sigma), label='Gaussian prior')\n",
    "plt.plot(x,np.ones(len(x)), label='Uniform prior')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('probability density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The devil's in the denominator\n",
    "\n",
    "$$P(\\theta | data) = \\frac{P(data|\\theta) P(\\theta)}{\\color{red}{P(data)}}$$\n",
    "\n",
    "- represents the probability of obtaining our particular sample of data, given a particular model (i.e. likelihood) and prior\n",
    "- **normalises** the posterior so the area under the probability distribution is 1\n",
    "\n",
    "\\begin{align*}\n",
    "P(data) &= \\int_{all \\theta} P(data, \\theta) d\\theta \\\\\n",
    "        &= \\int_{all \\theta} P(data|\\theta) P(\\theta) d\\theta\n",
    "\\end{align*}\n",
    "\n",
    "- Very difficult to calculate for all but low-dimensional $\\theta$, this is the motivation behind all the MCMC methods discussed in Lecture 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Posteriors: the goal of Bayesian inference\n",
    "\n",
    "$$\\color{red}{P(\\theta | data)} = \\frac{P(data|\\theta) P(\\theta)}{P(data)}$$\n",
    "\n",
    "**For example:**\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- How likely is our coin to be biased?\n",
    "- We perform an experiment of 10 flips, and it lands heads up 7 times\n",
    "- We will use the Uniform prior given earlier\n",
    "- Likelihood is:\n",
    "\n",
    "$$P(7\\times H | \\theta) = \\theta^7 (1-\\theta)^3$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We run two different experiments, one with 10 flips, another with 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# experiment 1 - 10 flips\n",
    "theta = np.linspace(0, 1, 1000)\n",
    "prior = np.ones(len(theta))\n",
    "likelihood = theta**7 * (1-theta)**3\n",
    "denominator = np.trapz(likelihood*prior, x=theta)\n",
    "posterior_exp1 = likelihood * prior / denominator\n",
    "\n",
    "# experiment 2 - 100 flips\n",
    "likelihood = theta**70 * (1-theta)**30\n",
    "denominator = np.trapz(likelihood*prior, x=theta)\n",
    "posterior_exp2 = likelihood * prior / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta,posterior_exp1, label='experiment 1')\n",
    "plt.plot(theta,posterior_exp2, label='experiment 2')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('probability density')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Inference on Noisy Time Series (PINTS)\n",
    "\n",
    "- **Authors:** Michael Clerx, Martin Robinson, Ben Lambert, Chon Lok Lei, Sanmitra Ghosh, Gary R. Mirams, David J. Gavaghan\n",
    "- An **open-source** (BSD 3-clause license) **Python** library that provides researchers with a broad suite of non-linear optimisation and sampling methods\n",
    "- Uses a **Bayesian framework**: Many different Priors, Likelihood functions, MCMC samplers and non-linear optimisers available\n",
    "- Use your *own pre-build model* for inference: users wrap their model and data in a transparent and straightforward interface\n",
    "- Pre-print available on [arXiv](https://arxiv.org/abs/1812.07388)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation and architecture\n",
    "\n",
    "PINTS is designed around two core ideas: \n",
    "\n",
    "1. PINTS should work with a wide range of time series models, and make no demands on how they are implemented other than a minimal input/output interface. \n",
    "2. It is assumed that model evaluation (simulation) is the most costly step in any optimisation or sampling routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Class Hierarchy](fig/class-hierarchy-eps-converted-to.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing a model\n",
    "\n",
    "- Pints is intended to work with a wide range of models, and assumes as little as possible about the model's form.\n",
    "- a \"model\" in Pints is anything that implements the `ForwardModel` interface:\n",
    "    - take a parameter vector $(\\boldsymbol{\\theta})$ and a sequence of times $(\\mathbf{t})$ as an input, \n",
    "    - and then return a vector of simulated values $(\\mathbf{y})$:\n",
    "\n",
    "$$f(\\boldsymbol{\\theta}, \\mathbf{t}) \\rightarrow \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Model\n",
    "\n",
    "In the example below, we define a system of ODEs (modelling a simple chemical reaction) and use SciPy to solve it. We then wrap everything in a `pints.ForwardModel class`, and use a Pints optimisation to find the best matching parameters.\n",
    "\n",
    "In this example we'll use a model of a reversible chemical reaction:\n",
    "\n",
    "$$\\dot{y}(t) = k_1 (1 - y) - k_2 y,$$\n",
    "\n",
    "where $k_1$ represents a forward reaction rate, $k_2$ is the backward reaction rate, and $y$ represents the concentration of a chemical solute.\n",
    "\n",
    "The next slide shows how you would implement this model using the standard Python package SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# Define the right-hand side of a system of ODEs\n",
    "def r(y, t, p):\n",
    "    k1 = p[0] # Forward reaction rate\n",
    "    k2 = p[1] # Backward reaction rate\n",
    "    dydt = k1 * (1 - y) - k2 * y\n",
    "    return dydt\n",
    "\n",
    "# Run an example simulation\n",
    "p = [5, 3]    # parameters\n",
    "y0 = 0.1      # initial conditions\n",
    "\n",
    "# Call odeint, with the parameters wrapped in a tuple\n",
    "times = np.linspace(0, 1, 1000)\n",
    "values = odeint(r, y0, times, (p,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing a wrapper class for Pints\n",
    "\n",
    "Now we'll wrap the model in a class that extends `pints.ForwardModel`.\n",
    "\n",
    "It should have two methods:\n",
    "  - `simulate()`: Run a simulation with the given parameters for the given times and return the simulated values\n",
    "  - `n_parameters()`: Return the dimension of the parameter vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "\n",
    "class ExampleModel(pints.ForwardModel):\n",
    "    \n",
    "    def simulate(self, parameters, times):\n",
    "        y0 = 0.1\n",
    "        def r(y, t, p):\n",
    "            dydt = (1 - y) * p[0] - y * p[1]\n",
    "            return dydt\n",
    "        return odeint(r, y0, times, (parameters,)).reshape(times.shape)\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return 2\n",
    "\n",
    "# Then create an instance of our new model class\n",
    "model = ExampleModel()\n",
    "\n",
    "# Run the same simulation using our new model wrapper\n",
    "values = model.simulate([5, 3], times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running an optimisation problem\n",
    "\n",
    "Now that our model implements the `pints.ForwardModel` interface, we can use it with Pints tools such as optimisers or MCMC.\n",
    "\n",
    "First, we use the model to generate test data by adding some generated noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the 'true' parameters\n",
    "true_parameters = [5, 3]\n",
    "\n",
    "# Run a simulation to get test data\n",
    "values = model.simulate(true_parameters, times)\n",
    "\n",
    "# Add some noise\n",
    "values += np.random.normal(0, 0.02, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the test data\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error function\n",
    "\n",
    "We then define a score function that characterises the mismatch between model predictions and data. \n",
    "\n",
    "We will use the classic sum of squares error for this (This is related to maximising a Bayesian Likelihood function with indepedent Gaussian Noise, see next Lecture). \n",
    "\n",
    "$$\\sum_{i=0}^N (f(\\boldsymbol{\\theta}, t_i) - f^d_i)^2$$\n",
    "\n",
    "where $\\mathbf{f}^d$ is the vector of test data. We can tell Pints about the the test data $\\mathbf{f}^d$ by wrapping it and the model $f(\\boldsymbol{\\theta}, \\mathbf{t})$ in a `pints.SingleOutputProblem` object \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "We then use the SNES optimiser to estimate the model parameters from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Select a score function\n",
    "score = pints.SumOfSquaresError(problem)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0.1, 0.1], [10, 10])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [1, 1]\n",
    "\n",
    "# Perform an optimization using SNES. \n",
    "found_parameters, found_value = pints.optimise(score, x0, boundaries=boundaries, method=pints.SNES)\n",
    "print('Score at true solution:')\n",
    "print(score(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values, alpha=0.5, label='noisy signal')\n",
    "plt.plot(times, problem.evaluate(found_parameters), label='recovered signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
