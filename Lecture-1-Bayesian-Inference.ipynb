{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Bayesian Inference and PINTS\n",
    "\n",
    "by Martin Robinson\n",
    "\n",
    "\n",
    "- Oxford Research Software Engineering group (http://www.cs.ox.ac.uk/projects/RSE)\n",
    "- Department of Computer Science, University of Oxford\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](fig/about_me.svg)\n",
    "\n",
    "- PhD in Mathematical Science at Monash University\n",
    "- Postdoc at the University of Twente, Netherlands\n",
    "- Currently:\n",
    "   - Senior Research Software Engineer in the [Oxford RSE group](http://www.cs.ox.ac.uk/projects/RSE)\n",
    "   - Co-director of EPSRC & MRC [Sustainable Approaches to Biomedical Science Centre for Doctoral Training: Responsible and Reproducible Research](https://sabsr3.web.ox.ac.uk), or SABS:R3 \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Research Interests include:\n",
    "\n",
    "- numerical modelling and simulation\n",
    "- particle-based methods\n",
    "- Bayesian inference\n",
    "- developing robust and reliable software for research\n",
    "     - [Aboria](https://github.com/aboria/Aboria) (C++ library for particle-based modelling methods)\n",
    "     - [Chaste](https://chaste.cs.ox.ac.uk) (C++ library for electrophysiology and discrete cellular modelling) \n",
    "     - [PINTS](https://github.com/pints-team/pints) (Python library for Parameter Inference for Time-Series Models)\n",
    "     - [Smoldyn](http://www.smoldyn.org/) (computer program for cell-scale biochemical simulations)\n",
    "     - [SPH-DEM](https://github.com/martinjrobins/SPH-DEM) (C++ library for coupled Smoothed Particle Hydrodynamics and Discrete Elements Modelling)\n",
    "     - [PyBaMM](https://github.com/tinosulzer/PyBaMM/tree/master/pybamm) (Python library for modelling batteries)\n",
    "     - [Trase](https://github.com/trase-cpp/trase) (C++ animated plotting library)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course Structure\n",
    "\n",
    "- **Lecture 1: Introduction to Bayesian Inference and Pints**\n",
    "   - **What is Bayesian Inference?**\n",
    "   - **Bayes Theorem: Priors, Likelihood functions and Posteriors**\n",
    "   - **Introduction to PINTS**\n",
    "   - **Using your own models in PINTS**\n",
    "- Lecture 2: Maximum Likelihood Estimation\n",
    "- Lecture 3: MCMC sampling\n",
    "- Lecture 4: Hierarchical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Target audience:\n",
    "\n",
    "- Researchers who want to apply statistical inference in their work.\n",
    "- toy models used to illustrate concepts, but with an emphisis on *time-series* models\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- A basic knowledge of mathematical programming in Python, or similar.\n",
    "- some basic statistics (probability distributions - continuous & discrete) and calculus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slides\n",
    "\n",
    "- Lecture slides written as a Jupyter notebook (https://jupyter.org/)\n",
    "- Converted to slides using the RISE plugin (https://github.com/damianavila/RISE)\n",
    "- *Almost all* plots in these lectures generated using Python code embedded in the notebook\n",
    "- Some example code is shown in the slides, for the rest you can look in the notebook\n",
    "   - Get all material here: https://github.com/pints-team/electrochemistry_course\n",
    "   - Need to install PINTs to execute the notebooks, instructions here: https://github.com/pints-team/pints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "- Most of the material for this course was taken from the textbook and lecture series by Dr Ben Lambert\n",
    "   - *Lambert, B. (2018). A Student’s Guide to Bayesian Statistics. Sage.* \n",
    "   - [https://ben-lambert.com](https://ben-lambert.com)\n",
    "   - Ben Lambert is also a fellow [PINTS](https://github.com/pints-team/pints) developer\n",
    "\n",
    "![](fig/lambert_book.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A conceptual introduction to inference\n",
    "\n",
    "### The Big world\n",
    "1. Consider an observable characteristic we are trying to\n",
    "explain, for example the heights of 5 randomly chosen\n",
    "individuals.\n",
    "2. Assume that there exists a true process $T$ that generates\n",
    "the heights of all individuals in our sample.\n",
    "3. There is variability in the observables outputted by $T$; this\n",
    "can due to the inherent\n",
    "variability or because we lack knowledge\n",
    "of the genetics and environmental factors that affect\n",
    "growth.\n",
    "4. Imagine a set of all conceivable processes that could result\n",
    "in our sample of height observations, which we call the\n",
    "“Big World”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "![](fig/big_world.svg)\n",
    "</center>\n",
    "\n",
    "*Images adapted from “A Technical Introduction to Probability and\n",
    "Bayesian Inference for Stan Users”, Stan Development Team, 2016.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is inference?\n",
    "\n",
    "- **Motivation:** update our knowledge of $T$ in light of data,\n",
    "and use the updated knowledge to estimate quantities of\n",
    "interest.\n",
    "  - In our height example we might want to estimate the\n",
    "mean height of the entire population having witnessed our\n",
    "sample of 5 individuals.\n",
    "\n",
    "### Method:\n",
    "1. Find areas of the Big World that are closest to $T$; ideally\n",
    "we would find $T$ itself!\n",
    "2. Estimate quantities of interest using these subsets of the\n",
    "*Small World*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Small World\n",
    "\n",
    "1. The infinity of the Big World is too large to be useful.\n",
    "2. Instead we first consider a subset of possible data\n",
    "generating processes which we call the “Small World”, or\n",
    "$\\Omega$.\n",
    "3. The Small World corresponds to a single probability model\n",
    "framework; in our height example we might suppose that\n",
    "$H \\sim  N(\\mu, \\sigma)$, where $\\mu$ is the mean height, and $\\sigma$ is their\n",
    "standard deviation.\n",
    "4. By varying our parameters $\\theta = (\\mu, \\sigma)$ we get different data\n",
    "generating processes.\n",
    "5. The collection of probability distributions we get by varying\n",
    "$\\theta \\in \\Omega$ in the Small World is known as the *Likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "![](fig/small_world.svg)\n",
    "</center>\n",
    "\n",
    "*An unlikely Small World*\n",
    "\n",
    "\n",
    "<center>\n",
    "    \n",
    "![](fig/small_world2.svg)\n",
    "</center>\n",
    "\n",
    "*A Boxian Small World: “All models are wrong but some are useful” (George Box, 1976)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Prior\n",
    "\n",
    "1. The Small World is still too big for our purposes.\n",
    "2. We usually have some knowledge about which areas of the\n",
    "Small World are nearest to $T$. For example we don’t\n",
    "believe that $\\mu = 100$m and $\\mu = 1.5$m are equally probable.\n",
    "3. As such, in *Bayesian* inference we define a prior probability\n",
    "density that gives a weighting to all $\\theta \\in \\Omega$ reflecting our\n",
    "beliefs.\n",
    "4. *Frequentist* inference does not require us to specify a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "![](fig/the_prior.svg)\n",
    "*A Gaussian prior for $\\Omega$*\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The data\n",
    "\n",
    "1. Inference is the process of updating our prior knowledge in\n",
    "light of data.\n",
    "2. In Bayesian inference with a likelihood and our prior\n",
    "knowledge explicitly stated we use *Bayes’ rule* to find our\n",
    "posterior probability density over $\\theta \\in \\Omega$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "![](fig/posterior.svg)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of the inference process\n",
    "\n",
    "1. Define the observables: The Big World\n",
    "2. Specify a likelihood (Small World)\n",
    "3. Specify a prior\n",
    "4. Input the data, calculate the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangible benefits of Bayesian inference\n",
    "\n",
    "- Straightforward application to scientific modelling and experimentatal data analysis\n",
    "- Simple and intuitive model building (unlike frequentist statistics there is no need to remember lots of specific formulae).\n",
    "- Exhaustive and creative model testing.\n",
    "- Straightforward interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentist and Bayesian world views\n",
    "\n",
    "The two paradigms differ in their definition of probability:\n",
    "\n",
    "**Frequentist:** $\\;P(A)\\;$ describes the limiting frequency of an event $A$. \n",
    " - there is a fixed value of $\\;P(A)\\;$ that must be calculated \n",
    "   - e.g. proportion of heads from a fair coin toss will approach 0.5 after a large number of trials. \n",
    " - Does not make sense to “update” probabilities.\n",
    "\n",
    "**Bayesian:** $\\;P(A)\\;$ is a measure of centainty, quantification of investigators belief that $\\;A\\;$ is true. \n",
    " - A fixed value of $\\;P(A)\\;$ is not neccessary, nor desirable. \n",
    " - Pior information must be used to augment sample data\n",
    " - $\\rightarrow$ free to update our beliefs using *Bayes’ rule*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "- Inference involves finding parameter values, or distributions of parameters values, for which model outputs are consistent with observations\n",
    "\n",
    "- Bayesian inference uses Bayes' rule to update prior beliefs after obtaining new data $y$\n",
    "\n",
    "```\n",
    "                     Bayes' Rule\n",
    "likelihood + prior -------------------> posterior\n",
    "```\n",
    "\n",
    " - **Likelihood function:** the probability of obtaining the data $y$, given a set of parameters $\\theta$\n",
    " - **Prior probability distribution:** encodes your uncertainty in the parameters before the data $y$ has been obtained\n",
    " - **Posterior distribution:** updated probability distribution of $\\theta$, given the new data $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Rule:\n",
    "\n",
    "<center>\n",
    "<figure> \n",
    "    <img src=\"fig/bayes_rule.svg\"> \n",
    "    <figcaption><i>An Essay towards solving a Problem in the Doctrine of Chances (1763)</i>\n",
    "    </figcaption> \n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "![Thomas Bayes](fig/225px-Thomas_Bayes.gif)\n",
    "*Reverend Thomas Bayes (1701?–1761)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Likelihoods\n",
    "\n",
    "$$P(\\theta | data) = \\frac{\\color{red}{{P(data|\\theta)}} P(\\theta)}{P(data)}$$\n",
    "\n",
    "- common to both Frequentist and Bayesian analyses\n",
    "- probability of generating the particular sample of data, given the model parameters $\\theta$\n",
    "- normally easy to obtain, *given a good statistical model*\n",
    "- often only interested in the *most likely* parameters (Maximum Likelihood Estimation, see Lecture 2)\n",
    "- **The aim of inference:** inverting the likelihood\n",
    " - Both Frequentists and Bayesians essentially invert:\n",
    " \n",
    "$$P(data|\\theta) \\rightarrow P(\\theta | data).$$\n",
    "\n",
    " - This amounts to going from an ’effect’ back to a ’cause’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: flipping a coin\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "Take the classic example of tossing a fair coin that has a probability landing heads up of $\\theta = 0.5$\n",
    "\n",
    "If we flip the coin 2 times, we have the set of possible outcomes:\n",
    "\n",
    "$$P(H, H | \\theta = 0.5) = P(H|\\theta=0.5) P(H|\\theta=0.5) = 0.25$$\n",
    "$$P(H, T | \\theta = 0.5) = P(H|\\theta=0.5) P(T|\\theta=0.5) = 0.25$$\n",
    "$$P(T, H | \\theta = 0.5) = P(T|\\theta=0.5) P(H|\\theta=0.5) = 0.25$$\n",
    "$$P(T, T | \\theta = 0.5) = P(T|\\theta=0.5) P(T|\\theta=0.5) = 0.25$$\n",
    "\n",
    "This is a valid probability distribution:\n",
    "\n",
    "$$ P(H, H | \\theta = 0.5) + P(H, T | \\theta = 0.5) + P(T, H | \\theta = 0.5)+ P(T, T | \\theta = 0.5) = 1.0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating the likelihood\n",
    "\n",
    "- Hold the **data constant**, and find the likelihood of this data given a certain $\\theta$\n",
    "- Provides an infinite number of possibilities to arrive at the given data\n",
    "- Take for example the case of $\\text{data} = H, H$\n",
    "- The likelihood function is given by $P(data|\\theta) = \\theta^2$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_coin_likelihood_hh():\n",
    "    theta = np.linspace(0,1,100)\n",
    "    likelihood = theta*theta\n",
    "    plt.plot(theta,likelihood)\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel('likelihood')\n",
    "    plt.fill_between(theta,likelihood,alpha=0.2)\n",
    "    plt.show()\n",
    "    return likelihood,theta\n",
    "\n",
    "def show_coin_likelihood_ht():\n",
    "    theta = np.linspace(0,1,100)\n",
    "    likelihood = theta*(1-theta)\n",
    "    plt.plot(theta,likelihood)\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel('likelihood')\n",
    "    plt.fill_between(theta,likelihood,alpha=0.2)\n",
    "    plt.show()\n",
    "    return likelihood,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "likelihood,theta = show_coin_likelihood_hh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is **not** a valid probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('integral under the likelihood = ',np.trapz(likelihood,x=theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different data this time\n",
    "\n",
    "- Take for example the case of $\\text{data} = H, T$\n",
    "- The likelihood function is given by $P(data|\\theta) = \\theta (1 - \\theta)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "likelihood,theta = show_coin_likelihood_ht()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Priors\n",
    "\n",
    "$$P(\\theta | data) = \\frac{{P(data|\\theta)} \\color{red}{P(\\theta)}}{P(data)}$$\n",
    "\n",
    "- This is particular to Bayesian inference, where we always update our *prior* beliefs using the new data\n",
    "- This is a function of all the possible parameter values $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Prior\n",
    "\n",
    "Back to the coin example, we know that the possible domain for our parameter is $\\theta \\in [0, 1]$.\n",
    "1. One option is the consider all the possible values of $\\theta$ to be equally likely (i.e. a Uniform prior).\n",
    "2. Another is to use our previous knowledge that most coins are likely to be fair (i.e. a Gaussian prior around $\\theta = 0.5$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "def show_priors():\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    ax1.plot(x,scipy.stats.uniform.pdf(x, loc=0, scale=1), label='Uniform prior')\n",
    "    ax1.fill_between(x,scipy.stats.uniform.pdf(x, loc=0, scale=1),alpha=0.2)\n",
    "\n",
    "    ax2.plot(x,scipy.stats.norm.pdf(x, 0.5, 0.1), label='Gaussian prior')\n",
    "    ax2.fill_between(x,scipy.stats.norm.pdf(x, 0.5, 0.1),alpha=0.2)\n",
    "\n",
    "    ax1.set_xlabel(r'$\\theta$')\n",
    "    ax1.set_ylabel('probability density')\n",
    "    ax2.set_xlabel(r'$\\theta$')\n",
    "    ax2.set_ylabel('probability density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_priors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The devil's in the denominator\n",
    "\n",
    "$$P(\\theta | data) = \\frac{P(data|\\theta) P(\\theta)}{\\color{red}{P(data)}}$$\n",
    "\n",
    "- represents the probability of obtaining our particular sample of data, given a particular model (i.e. likelihood) and prior\n",
    "- **normalises** the posterior so the area under the probability distribution is 1\n",
    "\n",
    "\\begin{align*}\n",
    "P(data) &= \\int_{all \\theta} P(data, \\theta) d\\theta \\\\\n",
    "        &= \\int_{all \\theta} P(data|\\theta) P(\\theta) d\\theta\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the denominator in 1 dimension\n",
    "\n",
    "For our coin example there is a single parameter $\\theta \\rightarrow P(H,H |\\theta) = \\theta^2$\n",
    "\n",
    "$$\n",
    "P(H,H) = \\int_0^1 P(H,H |\\theta) P(\\theta) d \\theta= \\int_0^1 \\theta^2 d \\theta = \\frac{1}{3} \\theta^3\n",
    "$$\n",
    "\n",
    "This is equivalent to working out an area under a curve. As you have already seen this is easy to calculate numerically as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the denominator in 2 dimension\n",
    "\n",
    "If we considered a different model where there were two\n",
    "parameters $\\theta_1 \\in (0, 1), \\theta_2 \\in (0, 1) \\rightarrow$:\n",
    "\n",
    "$$\n",
    "P(data) = \\int_0^1 \\int_0^1 P(data |\\theta) P(\\theta) d \\theta_1 d \\theta_2\n",
    "$$\n",
    "\n",
    "This is equivalent to working out a **volume** contained within a surface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the denominator in d dimensions\n",
    "\n",
    "If we considered a different model where there were $d$\n",
    "parameters ($\\theta_1, \\theta_2, ..., \\theta_d) \\in (0, 1) \\rightarrow$:\n",
    "\n",
    "$$\n",
    "P(data) = \\int_0^1 ... \\int_0^1 P(data |\\theta) P(\\theta) d \\theta_1 ... d \\theta_d\n",
    "$$\n",
    "\n",
    "This is equivalent to working out a ($d$ + 1)-dimensional **volume** contained within a $d$-dimensional (hyper-surface)! \n",
    "\n",
    "<center>\n",
    "<img src=\"fig/I_have_no_idea_dog.jpeg\"> \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The difficult denominator\n",
    "\n",
    "- Calculating the denominator possible for $d < 10$ using computers.\n",
    "- Numerical quadrature and many other approximate schemes struggle for larger $d$.\n",
    "- Many models have thousands of parameters.\n",
    "\n",
    "Arrrghhh!\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "- **Non-linear optimisation methods**: find the maximum of the posterior, either ignoring the rest of the posterior distribution or making a Gaussian assumption around the maximum point (Lecture 2)\n",
    "\n",
    "- **Sampling:** understand a distribution by sampling from it, rather than exact calculation (Lecture 3).\n",
    "\n",
    "- **Conjugate priors:** use certain pairs of distributions where we can follow basic rules to find posteriors, no calculation necessary! (Lecture 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Posteriors: the goal of Bayesian inference\n",
    "\n",
    "$$\\color{red}{P(\\theta | data)} = \\frac{P(data|\\theta) P(\\theta)}{P(data)}$$\n",
    "\n",
    "**For example:**\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- How likely is our coin to be biased?\n",
    "- We perform an experiment of 10 flips, and it lands heads up 7 times\n",
    "- We will use the Uniform prior given earlier\n",
    "- Likelihood is:\n",
    "\n",
    "$$P(7\\times H | \\theta) = \\theta^7 (1-\\theta)^3$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Lets run a number of different experiments, each using a different number of flips $f$, giving a different random number of heads $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# experiment 1 - 10 flips\n",
    "def run_coin_experiment(f):\n",
    "    theta = np.linspace(0, 1, 1000)\n",
    "    h = np.random.poisson(f/2)\n",
    "    \n",
    "    # Uniform prior\n",
    "    prior = np.ones(len(theta))\n",
    "    \n",
    "    # Likelihood function\n",
    "    likelihood = theta**h * (1-theta)**(f-h)\n",
    "    \n",
    "    # Normalising factor (numerical integration)\n",
    "    denominator = np.trapz(likelihood*prior, x=theta)\n",
    "    \n",
    "    # Bayes rule\n",
    "    posterior = likelihood * prior / denominator\n",
    "    \n",
    "    plt.plot(theta,posterior, label='h = %s'%str(h))\n",
    "    plt.fill_between(theta,posterior,alpha=0.2)\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel('posterior')\n",
    "    plt.title('area under posterior is %f'%np.trapz(posterior, x=theta))\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f_widget = FloatSlider(value=7.0, min=1.0, max=200.0, step=1.0, continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(run_coin_experiment, f=f_widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- All methods of inference involve the subjective decision of defining the boundaries of the Small World (**likelihood**).\n",
    "- Likelihood determined by a combination of the *statistical model* **and** the *data*\n",
    "- Small World inference involves *inversion of the likelihood*.\n",
    "- Bayesians use **Bayes’ rule**, which requires us to specify a **prior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Inference on Noisy Time Series (PINTS)\n",
    "\n",
    "- **Authors:** Michael Clerx, Martin Robinson, Ben Lambert, Chon Lok Lei, Sanmitra Ghosh, Gary R. Mirams, David J. Gavaghan\n",
    "- An **open-source** (BSD 3-clause license) **Python** library that provides researchers with a broad suite of non-linear optimisation and sampling methods\n",
    "- Uses a **Bayesian framework**: Many different Priors, Likelihood functions, MCMC samplers and non-linear optimisers available\n",
    "- Use your *own pre-build model* for inference: users wrap their model and data in a transparent and straightforward interface\n",
    "- Pre-print available on [arXiv](https://arxiv.org/abs/1812.07388)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation and architecture\n",
    "\n",
    "PINTS is designed around two core ideas: \n",
    "\n",
    "1. PINTS should work with a wide range of time series models, and make no demands on how they are implemented other than a minimal input/output interface. \n",
    "2. It is assumed that model evaluation (simulation) is the most costly step in any optimisation or sampling routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Class Hierarchy](fig/class-hierarchy-eps-converted-to.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing a model\n",
    "\n",
    "- Pints is intended to work with a wide range of models, and assumes as little as possible about the model's form.\n",
    "- a \"model\" in Pints is anything that implements the `ForwardModel` interface:\n",
    "    - take a parameter vector $(\\boldsymbol{\\theta})$ and a sequence of times $(\\mathbf{t})$ as an input, \n",
    "    - and then return a vector of simulated values $(\\mathbf{y})$:\n",
    "\n",
    "$$f(\\boldsymbol{\\theta}, \\mathbf{t}) \\rightarrow \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Model\n",
    "\n",
    "In the example below, we define a system of ODEs (modelling a simple chemical reaction) and use SciPy to solve it. We then wrap everything in a `pints.ForwardModel class`, and use a Pints optimisation to find the best matching parameters.\n",
    "\n",
    "In this example we'll use a model of a reversible chemical reaction:\n",
    "\n",
    "$$\\dot{y}(t) = k_1 (1 - y) - k_2 y,$$\n",
    "\n",
    "where $k_1$ represents a forward reaction rate, $k_2$ is the backward reaction rate, and $y$ represents the concentration of a chemical solute.\n",
    "\n",
    "The next slide shows how you would implement this model using the standard Python package SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# Define the right-hand side of a system of ODEs\n",
    "def r(y, t, p):\n",
    "    k1 = p[0] # Forward reaction rate\n",
    "    k2 = p[1] # Backward reaction rate\n",
    "    dydt = k1 * (1 - y) - k2 * y\n",
    "    return dydt\n",
    "\n",
    "# Run an example simulation\n",
    "p = [5, 3]    # parameters\n",
    "y0 = 0.1      # initial conditions\n",
    "\n",
    "# Call odeint, with the parameters wrapped in a tuple\n",
    "times = np.linspace(0, 1, 1000)\n",
    "values = odeint(r, y0, times, (p,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing a wrapper class for Pints\n",
    "\n",
    "Now we'll wrap the model in a class that extends `pints.ForwardModel`.\n",
    "\n",
    "It should have two methods:\n",
    "  - `simulate()`: Run a simulation with the given parameters for the given times and return the simulated values\n",
    "  - `n_parameters()`: Return the dimension of the parameter vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "\n",
    "class ExampleModel(pints.ForwardModel):\n",
    "    \n",
    "    def simulate(self, parameters, times):\n",
    "        y0 = 0.1\n",
    "        def r(y, t, p):\n",
    "            dydt = (1 - y) * p[0] - y * p[1]\n",
    "            return dydt\n",
    "        return odeint(r, y0, times, (parameters,)).reshape(times.shape)\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return 2\n",
    "\n",
    "# Then create an instance of our new model class\n",
    "model = ExampleModel()\n",
    "\n",
    "# Run the same simulation using our new model wrapper\n",
    "values = model.simulate([5, 3], times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running an optimisation problem\n",
    "\n",
    "Now that our model implements the `pints.ForwardModel` interface, we can use it with Pints tools such as optimisers or MCMC.\n",
    "\n",
    "First, we use the model to generate test data by adding some generated noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the 'true' parameters\n",
    "true_parameters = [5, 3]\n",
    "\n",
    "# Run a simulation to get test data\n",
    "values = model.simulate(true_parameters, times)\n",
    "\n",
    "# Add some noise\n",
    "values += np.random.normal(0, 0.02, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the test data\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error function\n",
    "\n",
    "We then define a score function that characterises the mismatch between model predictions and data. \n",
    "\n",
    "We will use the classic sum of squares error for this (This is related to maximising a Bayesian Likelihood function with indepedent Gaussian Noise, see next Lecture). \n",
    "\n",
    "$$\\sum_{i=0}^N (f(\\boldsymbol{\\theta}, t_i) - f^d_i)^2$$\n",
    "\n",
    "where $\\mathbf{f}^d$ is the vector of test data. We can tell Pints about the the test data $\\mathbf{f}^d$ by wrapping it and the model $f(\\boldsymbol{\\theta}, \\mathbf{t})$ in a `pints.SingleOutputProblem` object \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "We then use the SNES optimiser to estimate the model parameters from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Select a score function\n",
    "score = pints.SumOfSquaresError(problem)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0.1, 0.1], [10, 10])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [1, 1]\n",
    "\n",
    "# Perform an optimization using SNES. \n",
    "found_parameters, found_value = pints.optimise(score, x0, boundaries=boundaries, method=pints.SNES)\n",
    "print('Score at true solution:')\n",
    "print(score(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Concentration')\n",
    "plt.plot(times, values, alpha=0.5, label='noisy signal')\n",
    "plt.plot(times, problem.evaluate(found_parameters), label='recovered signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- If you have a time-series model that can be wrapped in a python function, you can start using **PINTS**\n",
    "- Future lectures illustrate using PINTS for:\n",
    "    - *Maximum Likelihood* and *Maximum a Postiori Estimation* (Lecture 2)\n",
    "    - *Markov Chain Monte Carlo* sampling (Lecture 3) and \n",
    "    - *Hierarchical models* (Lecture 4)\n",
    "- I will be at Monash for the next 2 weeks. *Very happy* to help if you need help getting PINTS working with your own models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
