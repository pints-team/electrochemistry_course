{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider, IntSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating independent samples:\n",
    "\n",
    "**Question:** how can we generate independent samples from the following (un-normalised) PDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def P(x):\n",
    "    return 0.2*norm.pdf(x,0.4,0.2) + 0.4*norm.pdf(x,1,0.2)+ 0.3*norm.pdf(x,1.6,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_pdf():\n",
    "    x = np.linspace(0,2,100)\n",
    "    y = P(x)\n",
    "    plt.plot(x,y)\n",
    "    plt.fill_between(x,y,alpha=0.2)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$P(x)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Answer:** do the following a large number of times:\n",
    "1. Generate $x$ coordinates: uniformly-distributed points from $(0, 2)$; where $2$ is the domain of the function.\n",
    "2. Generate $y$ coordinates: uniformly-distributed points from $(0, 1)$; where 1 is the maximum value of the function.\n",
    "3. If $y < P(x)$, then accept $x$ coordinate as a sample.\n",
    "4. If $y \\ge P(x)$, then reject $x$ coordinate as a sample.\n",
    "Known as **Rejection** sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 100000\n",
    "x = np.random.uniform(0,2,N)\n",
    "y = np.random.uniform(0,1,N)\n",
    "x_samples = x[y < P(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(x_samples,bins=100,density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do sampling in the first place?\n",
    "\n",
    "Samples from the posterior:\n",
    "\n",
    "$$P(\\theta | data) = \\frac{P(data|\\theta) P(\\theta)}{P(data)}$$\n",
    "\n",
    "- Shows the likely *distribution* of parameters, rather than single most likely point. Shows:\n",
    " - Correlated parameters\n",
    " - Unidentifiable parameters\n",
    " - Confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do sampling in the first place?\n",
    "\n",
    "- Often, want to calculate the posterior mean of some parameter, $\\theta_1$:\n",
    "    \n",
    "$$\n",
    "E(\\theta_1|X) = \\int_{\\theta_1} \\int_{\\theta_{-1}} \n",
    "\\theta_1  P(\\theta_1, \\theta_{-1} | X)d \\theta_{-1} d \\theta_1\n",
    "$$\n",
    "\n",
    "where $\\theta_{-1}$ corresponds to the $d − 1$ other parameters of the\n",
    "model.\n",
    "\n",
    "- Posterior mean can be approximated by summing over posterior samples\n",
    "\n",
    "- Marginal distributions can also be obtained from posterior samples:\n",
    "\n",
    "$$\n",
    "P(\\theta_1|X) = \\int_{\\theta_{-1}} P(\\theta_1, \\theta_{-1} | X)d \\theta_{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is generating independent samples difficult?\n",
    "\n",
    "- **Rejection sampling** requires generation of a large number\n",
    "of random points to produce relatively few samples.\n",
    "- This inefficiency increases (exponentially) with the\n",
    "dimensionality of the distribution; i.e. for posteriors with\n",
    "more parameters.\n",
    "- Other methods exist (inverse-transform sampling and\n",
    "importance sampling, for example) but they suffer from\n",
    "complexity and/or inefficiency issues.\n",
    "- We cannot calculate the denominator so are unable to use\n",
    "some of these methods.\n",
    "- Even if we had the denominator the complexity of most\n",
    "models means that independent sampling isn’t possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is sampling finished?\n",
    "\n",
    "<center>\n",
    "<img src=\"fig/sad-clown.jpg\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is dependent sampling?\n",
    "\n",
    "- A sampling algorithm where the next sample depends on the current value, and the list of all (accepted) positions of the\n",
    "sampler form the sample.\n",
    "\n",
    "**Example dependent sampler:** \n",
    "\n",
    "choose a new\n",
    "position based on a local “jumping” distribution\n",
    "Suppose the next value of the sampler is drawn from a 2d\n",
    "normal distribution centred on our current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show dependent sampler here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependent samplers as Markov Chains (Monte Carlo)\n",
    "\n",
    "- Where to step next is determined via a distribution\n",
    "conditional on the current parameter value.\n",
    "- This stepping is probabilistic $\\rightarrow$ *Monte Carlo*.\n",
    "- The conditional distribution only depends on the current\n",
    "value of the sampler meaning it is memoryless about past\n",
    "path.\n",
    "- This memoryless means that the path of the sampler is a\n",
    "*1st order Markov Chain*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Open questions\n",
    "\n",
    "How can we decide on the:\n",
    "1. Starting position.\n",
    "2. Jumping distribution’s shape.\n",
    "\n",
    "To ensure convergence to the posterior distribution? Especially\n",
    "because we cannot compute the posterior itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Walk Metropolis\n",
    "\n",
    "A discrete example: **David Robinson’s fishing**\n",
    "    \n",
    "- David Robinson (a more fortunate cousin of Robinson\n",
    "Crusoe) is marooned on an island.\n",
    "- Access to four freshwater lakes of different sizes; each with\n",
    "a supply of fish.\n",
    "\n",
    "<center>\n",
    "<img src=\"fig/david_robinson.svg\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## David Robinson’s fishing\n",
    "\n",
    "- He has a terrible memory (too much coconut toddy), and\n",
    "each day forgets any estimates of lake size he made\n",
    "previously.\n",
    "- He wants to fish (at maximum) one new lake per day.\n",
    "- He possesses a coin and a solar-powered calculator that\n",
    "can generate (pseudo-)random numbers uniformly\n",
    "distributed between 0 and 1.\n",
    "- He is initially “washed up” next to lake A.\n",
    "\n",
    "<center>\n",
    "<img src=\"fig/david_robinson.svg\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** What strategy should he use to fish as\n",
    "sustainably as possible?\n",
    "\n",
    "**Remember:** Robinson doesn’t know the # of lakes, nor\n",
    "the amount of fish in each!\n",
    "\n",
    "<center>\n",
    "<img src=\"fig/david_robinson.svg\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Answer:** visit each lake in proportion to the fish it contains, by\n",
    "doing the following:\n",
    "1. Each night he flips the coin.\n",
    "2. If it’s heads (tails) he proposes a move to the neighbouring\n",
    "lake in the clockwise (anticlockwise) direction.\n",
    "3. Calculates the ratio of the size of the proposed lake to the\n",
    "current one.\n",
    "4. Compares the ratio with a (pseudo-)random number from\n",
    "the calculator.\n",
    "5. If the ratio exceeds the generated number, he moves. If\n",
    "not, he stays put and fishes the same lake tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## David Robinson’s fishing: does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "x = [0, 1, 2, 3]\n",
    "lakes = [2.0, 1.0, 4.0, 2.0]\n",
    "days = np.zeros((4,N))\n",
    "current_lake = 0\n",
    "\n",
    "for i in range(1,N):\n",
    "    days[:,i] = days[:,i-1]\n",
    "    \n",
    "    # move either clockwise or counter-clockwise\n",
    "    proposed_lake = current_lake + 2*int(np.random.uniform() < 0.5) - 1\n",
    "    if proposed_lake > 3:\n",
    "        proposed_lake = 0;\n",
    "    elif proposed_lake < 0:\n",
    "        proposed_lake = 3\n",
    "        \n",
    "    # accept move based on the ratio of lake sizes\n",
    "    ratio = lakes[proposed_lake]/lakes[current_lake]\n",
    "    if ratio > np.random.uniform():\n",
    "        current_lake = proposed_lake\n",
    "        \n",
    "    days[current_lake, i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_david_robinson(i):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    ax1.plot(x, days[:,i], 'bo', ms=8, label='poisson pmf')\n",
    "    ax1.vlines(x, 0, days[:,i], colors='b', lw=5, alpha=0.5)\n",
    "    ax2.plot(x, lakes, 'bo', ms=8, label='poisson pmf')\n",
    "    ax2.vlines(x, 0, lakes, colors='b', lw=5, alpha=0.5)\n",
    "    ax1.set_ylim(0,N/2)\n",
    "    plt.show()\n",
    "\n",
    "day_widget = IntSlider(value=5, min=0, max=N-1, step=1, continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(plot_david_robinson, i=day_widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## David Robinson’s fishing: summary\n",
    "\n",
    "- Robinson lacked knowledge of numbers of fish in each lake and the number of lakes.\n",
    "- Only knows that the number of fish in each lake is proportionate to its size.\n",
    "- His memory stops him remembering the exact sizes.\n",
    "- After about 100 days his “random” strategy is quite similar from an “omniscient” one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Random Walk Metropolis\n",
    "\n",
    "Robinson’s strategy is an example of the “Random Walk Metropolis” algorithm. This has the following form:\n",
    "\n",
    "- Generate a random starting location $\\theta_0$.\n",
    "- Iterate the following for $t = 1, ..,T$:\n",
    "  - Propose a new location from a jumping distribution: \n",
    "$$\\theta_{t+1} \\sim J(\\theta_{t+1}|\\theta_t)$$.\n",
    "  - Calculate the ratio: \n",
    "$$r = \\frac{\\text{likelihood}(\\theta_{t+1}) \\times \\text{prior}(\\theta_{t+1})}{\\text{likelihood}(\\theta_t) \\times \\text{prior}(\\theta_t)}$$\n",
    "  - Compare $r$ with a uniformly-distributed number $u$ between 0 and 1.\n",
    "  - If $r \\ge u \\Rightarrow$ we move.\n",
    "  - Otherwise, we remain at our current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "T = 2000\n",
    "chain = np.empty(T,dtype=float)\n",
    "theta0 = 1.0\n",
    "accepted = 0\n",
    "for t in range(T):\n",
    "    # Propose a new location from a jumping distribution\n",
    "    theta1 = np.random.normal(theta0,0.2)\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    r = P(theta1)/P(theta0)\n",
    "    \n",
    "    # Compare r with a uniformly-distributed number between 0 and 1.\n",
    "    if r >= np.random.uniform():\n",
    "        theta0 = theta1\n",
    "    \n",
    "    # Add to chain of samples\n",
    "    chain[t] = theta0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_mcmc_chain(chain):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    ax1.plot(chain)\n",
    "    ax1.set_xlabel(r'iteration $t$')\n",
    "    ax1.set_ylabel(r'parameter $\\theta$')\n",
    "    ax2.hist(chain,bins=50,density=True)\n",
    "    ax2.set_ylabel(r'probability')\n",
    "    ax2.set_xlabel(r'parameter $\\theta$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_mcmc_chain(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCMC Sampling using PINTS\n",
    "\n",
    "- We can wrap our $P(x)$ PDF using PINTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "\n",
    "class MyPDF(pints.toy.ToyLogPDF): \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.log(P(x))\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return 1\n",
    "\n",
    "log_pdf = MyPDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCMC Sampling using PINTS\n",
    "\n",
    "- and then sample using MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create an MCMC routine\n",
    "x0 = [[1.0]]\n",
    "mcmc = pints.MCMCController(log_pdf, 1, x0, sigma0=0.2, method=pints.MetropolisRandomWalkMCMC)\n",
    "\n",
    "# Set maximum number of iterations\n",
    "mcmc.set_max_iterations(2000)\n",
    "\n",
    "# Run the method\n",
    "chains = mcmc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_mcmc_chain(chains[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Metropolis: benefits\n",
    "- Under quite general conditions the Random Walk\n",
    "Metropolis sampler converges asymptotically to the\n",
    "posterior.\n",
    "- However for a sufficiently large sample size the sampling\n",
    "distribution may be practically indistinguishable from the\n",
    "true posterior.\n",
    "- The algorithm requires us to be able to calculate the ratio:\n",
    "\n",
    "$$r = \\frac{\\text{likelihood}(\\theta_{t+1}) \\times \\text{prior}(\\theta_{t+1})}{\\text{likelihood}(\\theta_t) \\times \\text{prior}(\\theta_t)}$$\n",
    "\n",
    "- The ratio uses only the numerator of Bayes’ rule $\\Rightarrow$ we\n",
    "side-step calculating the denominator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The importance of the accept/reject rule\n",
    "\n",
    "- **Question:** what’s the function of the accept/reject rule\n",
    "we use in the Metropolis algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- allows sampling from\n",
    "each point in exact proportion to the posterior height\n",
    "\n",
    "- Consider the ratio of the posterior density at point θt+1 to θt\n",
    "\n",
    "\\begin{align*}\n",
    "r &= \\frac{P(\\theta_{t+1}|X)}{P(\\theta_t|X)} \\\\\n",
    "  &= \\frac{\\frac{P(X|\\theta_{t+1})P(\\theta_{t+1})}{P(X)}}{\\frac{P(X|\\theta_t)P(\\theta_t)}{P(X)}} \\\\\n",
    "  &= \\frac{P(X|\\theta_{t+1})P(\\theta_{t+1})}{P(X|\\theta_t)P(\\theta_t)}\n",
    "\\end{align*}\n",
    "\n",
    "So the ratio of the numerators of Bayes’ rule is identical to the\n",
    "ratio of the actual posteriors.\n",
    "\n",
    "$\\Rightarrow$ if we use r to guide our stepping we will be sampling\n",
    "(eventually) from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we choose the jumping distribution?\n",
    "\n",
    "- Sometimes called the “proposal distribution”.\n",
    "- In Random Walk Metropolis we use a symmetric\n",
    "distribution (relaxed in Metropolis-Hastings):\n",
    "\n",
    "$$=⇒ J(\\theta_a|\\theta_b) = J(\\theta_b|\\theta_a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_proposal_distribution():\n",
    "    theta_a = 1\n",
    "    theta_b = 2\n",
    "    x = np.linspace(0,3,100)\n",
    "    plt.plot(x,norm.pdf(x,theta_a))\n",
    "    plt.plot(x,norm.pdf(x,theta_b))\n",
    "    plt.plot([theta_a,theta_b],[norm.pdf(theta_a,theta_b),norm.pdf(theta_b,theta_a)], color='k')\n",
    "    plt.plot([theta_a, theta_a],[0, norm.pdf(theta_a,theta_b)], color='k', ls='--')\n",
    "    plt.plot([theta_b, theta_b],[0, norm.pdf(theta_b,theta_a)], color='k', ls='--')\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel(r'$J$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_proposal_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The importance of step size\n",
    "\n",
    "**Question:** how should we decide on the jumping kernel’s step\n",
    "size?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
