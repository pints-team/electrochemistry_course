{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider, IntSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 10\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 14\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course Structure\n",
    "\n",
    "- Lecture 1: Introduction to Bayesian Inference and Pints\n",
    "- Lecture 2: Maximum Likelihood Estimation\n",
    "- Lecture 3: MCMC sampling\n",
    "- **Lecture 4: Hierarchical models**\n",
    "    - **Conditional probability distributions**\n",
    "    - **Gibbs sampling**\n",
    "    - **Hierarchical model - lake example**\n",
    "    - **Implementation in PINTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional distributions\n",
    "\n",
    "### Example Two-dimensional distribution:\n",
    "- Imagine you are interested in the interrelation between the circumference of a person’s head ($H$) and the volume of their brain ($B$).\n",
    "- Based on data we find there is a positive correlation between these two variables, which we represent in a distribution $P(H, B)$.\n",
    "\n",
    "![](fig/Human-brain.SVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def brain_pdf(xy):\n",
    "    return multivariate_normal.pdf(xy, mean=[45, 1500], cov=[[10**2, 5*100],[5*100,200**2]])\n",
    "\n",
    "def get_full_brain_pdf():\n",
    "    x = np.linspace(20, 70, 50)\n",
    "    y = np.linspace(1000, 2000, 40)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XY = np.column_stack([X.flat, Y.flat])\n",
    "\n",
    "    Z = brain_pdf(XY).reshape(X.shape)\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "def get_conditional_brain_pdf():\n",
    "    x = np.linspace(20, 70, 50)\n",
    "    y = 1450*np.ones_like(x)\n",
    "    \n",
    "    xy = np.column_stack([x, y])\n",
    "    z = brain_pdf(xy).reshape(x.shape)\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def show_brain_distribution(conditional=False):\n",
    "    X, Y, Z = get_full_brain_pdf()\n",
    "    plt.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    \n",
    "    if conditional:\n",
    "        plt.plot([20,70],[1450, 1450])\n",
    "    plt.xlabel(r'$H$')\n",
    "    plt.ylabel(r'$B$')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def show_brain_distribution_conditional():   \n",
    "    X, Y, Z = get_full_brain_pdf()\n",
    "    x, y, z = get_conditional_brain_pdf()\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "\n",
    "    ax1.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    ax1.plot([20,70],[1450, 1450])\n",
    "    ax2.plot(x, z)\n",
    "    ax2.fill_between(x,z,alpha=0.2)\n",
    "    ax1.set_xlabel(r'$H$')\n",
    "    ax1.set_ylabel(r'$B$')\n",
    "    ax2.set_ylabel(r'$P(B = 1450, H)$')\n",
    "    ax2.set_xlabel(r'$H$')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_brain_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional distributions\n",
    "\n",
    "- **Question:** If an individual has a brain volume of $1450\\text{cm}^3$, then what does the distribution for their head circumference look like?\n",
    "- **Answer:** Use law of conditional probability:\n",
    "\n",
    "$$P(H|B = 1450) = \\frac{P(B = 1450, H)} {P(B = 1450)}$$\n",
    "\n",
    "- **Analogy:** imagine walking over the probability distribution along a line of $B = 1450\\text{cm}^3$, and recording your height as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_brain_distribution_conditional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs sampler\n",
    "\n",
    "- A dependent sampling technique\n",
    "- Useful for hierarchical models: breaks up higher-dimensional problems into separate lower-dimensional problems, *conditioned* on the remaining variables.\n",
    "- Can be used in conjunction with Random Walk Metropolis method $\\Rightarrow$ still allows us to use all the machinery we've already developed!\n",
    "- Named after Josiah Willard Gibbs (February 11, 1839 – April 28, 1903), algorithm proposed by Stuart and Donald Geman in 1984\n",
    "\n",
    "![](fig/Josiah_Willard_Gibbs_-from_MMS-.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Gibbs sampler\n",
    "\n",
    "For a parameter vector: $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$:\n",
    "\n",
    "- Select a random starting location: $(\\theta_1^0, \\theta_2^0, \\theta_3^0)$, along the same lines as for Random Walk Metropolis.\n",
    "- For each iteration $t = 1, ..., T$ do:\n",
    "    1. Select a random parameter update ordering, for example $(\\theta_3, \\theta_2, \\theta_1)$.\n",
    "    2. Independently sample from the conditional posterior for each parameter in order using the most up-to-date parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Gibbs sampler\n",
    "\n",
    "First we sample:\n",
    "    \n",
    "$$\\theta^1_3 \\sim P(\\theta_3|\\theta^0_2, \\theta_1^0)$$\n",
    "\n",
    "Then conditional on freshly-sampled $\\theta^1_3$\n",
    "\n",
    "$$\\theta^1_2 \\sim P(\\theta_2|\\theta^1_3, \\theta_1^0)$$\n",
    "\n",
    "Then conditional on freshly-sampled $\\theta^1_3$ and $\\theta_2^1$:\n",
    "\n",
    "$$\\theta^1_1 \\sim P(\\theta_1|\\theta^1_3, \\theta_2^1)$$\n",
    "\n",
    "**Important:** in Gibbs sampling there is no rejection of steps \n",
    "\n",
    "$\\Rightarrow$ unlike Random Walk Metropolis!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example application of Gibbs sampling: : speed of motion of neighbouring birds in a flock\n",
    "\n",
    "Suppose we record the speed of bird A ($v_A$) and bird B ($v_B$) in\n",
    "a flock along a particular axis.\n",
    "\n",
    "Based on observations we find that the joint posterior\n",
    "distribution over speeds is a multivariate normal distribution:\n",
    "\n",
    "\n",
    "$$\\begin{pmatrix} v_A \\\\ v_B \\end{pmatrix} \\sim N \\left [ \\begin{pmatrix} v_0 \\\\ v_0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right ]$$\n",
    "\n",
    "Of course here we have an analytic expression for the posterior distribution, but this example illustrates how the method works for more general problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bird_pdf(xy):\n",
    "    return multivariate_normal.pdf(xy, mean=[0, 0], cov=[[1, 0.5],[0.5,1]])\n",
    "\n",
    "def get_full_bird_pdf():\n",
    "    x = np.linspace(-5, 5, 50)\n",
    "    y = np.linspace(-5,5, 40)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XY = np.column_stack([X.flat, Y.flat])\n",
    "\n",
    "    Z = bird_pdf(XY).reshape(X.shape)\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "def show_bird_distribution():\n",
    "    X, Y, Z = get_full_bird_pdf()\n",
    "    plt.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    plt.xticks([0],['v_0'])\n",
    "    plt.yticks([0],['v_0'])\n",
    "    plt.xlabel('speed of bird A')\n",
    "    plt.ylabel('speed of bird B')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_bird_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the conditional distributions\n",
    "\n",
    "- In many circumstances we cannot find the conditional distributions however here it is possible.\n",
    "\n",
    "- If we knew $v_B$ :\n",
    "\n",
    "$$v_A \\sim N(v_0 + \\rho(v_B − v_0), 1 − \\rho^2)$$\n",
    "\n",
    "- Alternatively, if we knew $v_A$:\n",
    "\n",
    "$$v_B \\sim N(v_0 + \\rho(v_A − v_0), 1 − \\rho^2)$$\n",
    "\n",
    "Use Gibbs sampling to conditionally sample: $v_A|v_B$ then $v_B |v_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gibbs_sample(n):\n",
    "    v0 = 0\n",
    "    rho = 0.5\n",
    "    samples = np.empty((2,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        # sample from v_A\n",
    "        samples[0,i] = np.random.normal(v0 + rho * (samples[1,i-1] - v0), 1 - rho**2)\n",
    "        \n",
    "        # sample from v_B\n",
    "        samples[1,i] = np.random.normal(v0 + rho * (samples[0,i] - v0), 1 - rho**2)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_gibbs_sampling(n):\n",
    "    samples = gibbs_sample(n)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    X, Y, Z = get_full_bird_pdf()\n",
    "    ax1.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    ax1.scatter(samples[0,:],samples[1,:],alpha=0.6)\n",
    "    ax1.set_xticks([0],['v_0'])\n",
    "    ax1.set_yticks([0],['v_0'])\n",
    "    ax1.set_xlabel('speed of bird A')\n",
    "    ax1.set_ylabel('speed of bird B')\n",
    "    \n",
    "    ax2.hist2d(samples[0,:],samples[1,:],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(show_gibbs_sampling, n= IntSlider(value=10, min=0, max=6000, step=100, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Models\n",
    "\n",
    "- So far we have looked at time-series statistical models of the form:\n",
    "    \n",
    "$$z_i \\sim y(\\theta, t_i) + N(0, \\sigma)$$\n",
    "\n",
    "where $y(\\theta, t)$ is the solution of a differential equation subject to a set of parameters $\\theta$\n",
    "\n",
    "- but what if some (or all) of the parameters $\\theta$ were themselves drawn from a random distribution?\n",
    "    \n",
    "$$\\theta \\sim N(0, \\sigma_\\theta)$$\n",
    "\n",
    "- This type of multi-level statistical model is a *hierarchical model*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Models: An example\n",
    "\n",
    "- Imagine a set of $n$ lakes, each with its own population of algae. \n",
    "- You know from measurements $z^j_i$ that the algae concentration in lake $j$ is given by:\n",
    "    \n",
    "$$ \\frac{dy_j(t)}{dt} = r y_j(t) \\frac{k_j - y_j(t)}{k_j}$$\n",
    "$$z_i^j \\sim y_j(t_i) + N(0,\\sigma)$$\n",
    "\n",
    "where $k_j$ is the carrying capacity of each lake, and $r$ is the growth rate\n",
    "\n",
    "![](fig/lake.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You know that the growth rate $r$ is species dependent and therefore fixed. \n",
    "- But the carrying capacity $k_j$ varies according to the properties of each lake, and can be modelled by a Gaussian distribution\n",
    "\n",
    "$$ k_j \\sim N(k, \\sigma_k)$$\n",
    "\n",
    "![](fig/lake.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$ \\frac{dy_j(t)}{dt} = r y_j(t) \\frac{k_j - y_j(t)}{k_j}$$\n",
    "\n",
    "$$z_i^j \\sim y_j(t_i) + N(0,\\sigma)$$\n",
    "\n",
    "$$ k_j \\sim N(k, \\sigma_k)$$\n",
    "\n",
    "- The parameters $\\theta = [k_1, k_2, k_3, ..., k_n, r]$ are the parameters of our lower-level ODE model\n",
    "- The parameters $\\theta_h = [k, \\sigma_k]$ are the *hyper-parameters* of our model\n",
    "- We want to find our posterior over *all* of the parameters and hyper-parameters\n",
    "\n",
    "![](fig/lake.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Models: hyper priors\n",
    "\n",
    "- $[k, \\sigma_k]$ are parameters, so in a Bayesian context we need to assign them **priors**!\n",
    "- Priors for the hyper-parameters are $P(k, \\sigma_k)$, we will set this to be a *normal-inverse-gamma* distribution:\n",
    "\n",
    "$$P(k, \\sigma_k) = \\text{NIG}(\\mu, \\lambda, \\alpha, \\beta)$$\n",
    "\n",
    "- Why? the normal inverse gamma distribution is *conjugate* with the Gaussian distribution. This makes it easier to calculate the posterior, more on this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** what is the numerator of Bayes’ rule for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** the joint distribution of the data X and parameters; i.e. $P(X, \\theta, \\theta_h, \\mu, \\lambda, \\alpha, \\beta)$\n",
    "\n",
    "**Another question:** how do we find this here? \n",
    "\n",
    "**Another answer:** exploit conditional independence of the problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "Start with hyper-parameters, the \"population\" level parameters\n",
    "\n",
    "![](fig/hierachical1.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "And determine their joint probability.\n",
    "\n",
    "![](fig/hierachical2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "Find the probability of $\\theta$ **conditional** on $k$ and $\\sigma_k$.\n",
    "\n",
    "![](fig/hierachical3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "and then the probability of the data $X$ conditional on $\\theta$.\n",
    "\n",
    "![](fig/hierachical4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "Finally to obtain the overall probability, multiply together all these terms\n",
    "\n",
    "![](fig/hierachical6.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So the posterior is found as: \n",
    "    \n",
    "\\begin{align*}\n",
    "P(\\theta, k, \\sigma_k | X) &\\propto P(X, \\theta, k, \\sigma_k) \\\\\n",
    "& = \\color{red}{P(X | \\theta)} \\times \\color{blue}{P(\\theta | k, \\sigma_k)} \\times \\color{purple}{P(k, \\sigma_k)}\n",
    "\\end{align*}\n",
    "\n",
    "- $\\color{red}{P(X|\\theta)}$ is just the **likelihood**.\n",
    "- $\\color{blue}{P(\\theta|k, \\sigma_k)}$ is the **prior** on $\\theta$.\n",
    "- $\\color{purple}{P(k, \\sigma_k)}$ is the **hyper-prior** on the hyper-parameters $k$ and $\\sigma_k$.\n",
    "- However the word “hyper” is really just a fancy word we use to represent priors on “population” level parameters.\n",
    "- In hierarchical models there is a blurring of the likelihood/prior boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical model\n",
    "\n",
    "Our likelihood and prior are really many models, one for each lake or output value\n",
    "\n",
    "![](fig/hierachical5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- All these models at the same level of the hierarchy are independent, so out posterior now becomes\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\theta, k, \\sigma_k | X) &\\propto P(X, \\theta, k, \\sigma_k) \\\\\n",
    "& = \\color{red}{P(X | \\theta)} \\times \\color{blue}{P(\\theta | k, \\sigma_k)} \\times \\color{purple}{P(k, \\sigma_k)} \\\\\n",
    "& = \\color{red}{\\prod_i^N P(X_i | \\theta)} \\times \\color{blue}{P(r) \\prod_i^n P(k_i | k, \\sigma_k)} \\times \\color{purple}{P(k, \\sigma_k)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Next problem:** how do we calculate this? How do we sample from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Answer: Gibbs sampling\n",
    "\n",
    "- our posterior is:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\theta, k, \\sigma_k | X) &\\propto \\prod_i^N P(X_i | \\theta) \\times P(r) \\prod_i^n P(k_i | k, \\sigma_k) \\times P(k, \\sigma_k)\n",
    "\\end{align*}\n",
    "\n",
    "- We can use Gibbs sampling, combined with MCMC, to sample from this\n",
    "- First we write down the posterior conditioned on each of our parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional distributions\n",
    "\n",
    "1. Posterior distribution conditioned on $k$ and $\\sigma_k$\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\theta | k, \\sigma_k, X) &\\propto \\prod_i^N P(X_i | \\theta) \\times P(r) \\prod_i^n P(k_i | k, \\sigma_k) \\times P(k, \\sigma_k) \\\\\n",
    "&\\propto \\prod_i^N P(X_i | \\theta) \\times P(r) \\prod_i^n P(k_i | k, \\sigma_k)\n",
    "\\end{align*}\n",
    "\n",
    "2. Posterior distribution conditioned on $\\theta$\n",
    "\n",
    "\\begin{align*}\n",
    "P(k, \\sigma_k | \\theta, X) &\\propto \\prod_i^N P(X_i | \\theta) \\times P(r) \\prod_i^n P(k_i | k, \\sigma_k) \\times P(k, \\sigma_k) \\\\\n",
    "&\\propto \\prod_i^n P(k_i | k, \\sigma_k) \\times P(k, \\sigma_k)\n",
    "\\end{align*}\n",
    "\n",
    "**Question:** Can sample from 1. using MCMC as before. Could do the same for 2., but we can do better...how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Answer**: Make sure we are using a combination of likelihood and prior that are *conjugate* with each other\n",
    "\n",
    "- If the posterior and prior are the same distribution for a particular likelihood function, then the prior distribution is called a *conjugate prior* for that likelihood.\n",
    "- Another way to sample from the posterior (as long as you can sample from the prior)\n",
    "- **For example:** the normal-inverse-gamma distribution is a conjugate prior for the Gaussian distribution (See table [here](https://en.wikipedia.org/wiki/Conjugate_prior)), so\n",
    "\n",
    "\\begin{align*}\n",
    "P(k, \\sigma_k | \\theta, X) &\\propto \\prod_i^n P(k_i | k, \\sigma_k) \\times P(k, \\sigma_k) \\\\\n",
    " &\\propto \\prod_i^n \\text{N}(k_i, \\sigma_k) \\times \\text{NIG}(\\mu, \\lambda, \\alpha, \\beta) \\\\\n",
    " &\\propto \\text{NIG}\\left (\\frac{\\lambda \\mu + n \\overline{k}}{\\lambda + n}, \\lambda + n, \\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i=1}^N (k_i - \\overline{k})^2 + \\frac{n\\lambda}{\\lambda+n}\\frac{(\\overline{k} - \\mu)^2}{2} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\overline{k} = \\frac{1}{n} \\sum_{i=1}^n k_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in PINTS\n",
    "\n",
    "- now we have all the equations in place to start sampling from this posterior\n",
    "- the next slides will go through how to implement this in PINTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generate data\n",
    "\n",
    "- we will use the `LogisticModel` from PINTs.\n",
    "- we need to set the real parameters, and then generate some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "model = pints.toy.LogisticModel()\n",
    "\n",
    "n = 10\n",
    "noise = 20\n",
    "real_hyper_parameters = [500, 50]\n",
    "real_parameters = [\n",
    "    np.array([0.015, np.random.normal(real_hyper_parameters[0], real_hyper_parameters[1])]) \n",
    "        for i in range(n)\n",
    "]\n",
    "times = np.linspace(0, 800, 50)\n",
    "values = [\n",
    "    model.simulate(real, times) + np.random.normal(0, noise, times.shape)\n",
    "        for real in real_parameters\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Likelihoods $P(X | \\theta)$\n",
    "\n",
    "- need to create one for each lake\n",
    "- we will assume the measurement noise is known for each likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "problems = [\n",
    "    pints.SingleOutputProblem(model, times, v)\n",
    "        for v in values\n",
    "]\n",
    "\n",
    "log_likelihoods = [\n",
    "    pints.GaussianKnownSigmaLogLikelihood(problem, noise)\n",
    "        for problem in problems\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Priors $P(\\theta|k, \\sigma_k)$\n",
    "\n",
    "- Uniform prior for $r$ over a sensible range\n",
    "- Gaussian prior for $k_i$. The mean & standard deviation will be conditioned later during the Gibbs sampling, but we will set initial values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "log_priors = [\n",
    "    pints.ComposedLogPrior(\n",
    "        pints.UniformLogPrior([0.01],[0.02]),\n",
    "        pints.GaussianLogPrior(real_hyper_parameters[0],1e9*real_hyper_parameters[0])\n",
    "    )\n",
    "        for i in range(n)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCMC samplers\n",
    "\n",
    "- We will use random initial points for the MCMC samplers\n",
    "- Using Adaptive Covariance MCMC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "log_posteriors = [\n",
    "    pints.LogPosterior(log_likelihood, log_prior)\n",
    "        for log_likelihood, log_prior in zip(log_likelihoods,log_priors)\n",
    "]\n",
    "\n",
    "starting_points = [\n",
    "    [ real*np.random.uniform(0.9,1.1) ]\n",
    "    for real in real_parameters\n",
    "]\n",
    "\n",
    "samplers = [\n",
    "    pints.AdaptiveCovarianceMCMC(x0)\n",
    "     for x0 in starting_points\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initial phase for MCMC samplers\n",
    "\n",
    "- We will use the *ask-and-tell* interface for the MCMC samplers during Gibbs\n",
    "- Therefore, need to manually run the initial phase for each MCMC sampler (normally done automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_burn_in = 200\n",
    "[sampler.set_initial_phase(True) for sampler in samplers]\n",
    "for sample in range(n_burn_in):\n",
    "    for i, (sampler, log_posterior) in enumerate(zip(samplers, log_posteriors)):\n",
    "        x = sampler.ask()\n",
    "        sampler.tell(log_posterior(x))\n",
    "[sampler.set_initial_phase(False) for sampler in samplers];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional sampler 1.\n",
    "\n",
    "This function samples from the posterior distribution conditioned on $k$ and $\\sigma_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_sampler1(samplers, log_posteriors):\n",
    "    xs = np.empty((2, n))\n",
    "    error = 0\n",
    "    for i, (sampler, log_posterior) in enumerate(zip(samplers, log_posteriors)):\n",
    "        x = sampler.ask()\n",
    "        xs[:,i] = sampler.tell(log_posterior(x))\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional sampler 2.\n",
    "\n",
    "This function samples from the posterior distribution conditioned on $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.stats\n",
    "def conditional_sampler2(xs, mu_0, k_0, alpha_0, beta_0):\n",
    "    xhat = np.mean(xs[1,:])\n",
    "    var = np.sum((xs[1,:]-xhat)**2)\n",
    "\n",
    "    k = k_0 + n\n",
    "    mu = (k_0 * mu_0 + n * xhat) / float(k)\n",
    "    alpha = alpha_0 + n/2\n",
    "    beta = beta_0 + 0.5*(var + ((k_0 * n) / k) * (xhat - mu_0)**2)\n",
    "    \n",
    "    variance_sample = scipy.stats.invgamma.rvs(a=alpha, loc=0, scale=beta)\n",
    "    mean_sample = scipy.stats.norm.rvs(mu,np.sqrt(variance_sample / k))\n",
    "    \n",
    "    return mean_sample, variance_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs sampler\n",
    "\n",
    "Now we will put them all togeter and run our Gibbs sampler.\n",
    "\n",
    "First set the parameters for the hyper-prior and allocate space for the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# prior hyperparameters\n",
    "k_0 = 0\n",
    "mu_0 = 0\n",
    "alpha_0 = 0\n",
    "beta_0 = 0\n",
    "\n",
    "samples = 5000\n",
    "chain = np.empty((samples, 2))\n",
    "k_chains = [np.empty((samples, 2)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs sampler\n",
    "\n",
    "... then perform the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for sample in range(samples):\n",
    "    # sample from the posterior distribution conditioned on $k$ and $\\sigma_k$\n",
    "    xs = conditional_sampler1(samplers, log_posteriors)\n",
    "    \n",
    "    # sample from the posterior distribution conditioned on $\\theta$\n",
    "    mean_sample, variance_sample = conditional_sampler2(xs, mu_0, k_0, alpha_0, beta_0)\n",
    "\n",
    "    # store samples\n",
    "    for i in range(n):\n",
    "        k_chains[i][sample, :] = xs[:, i]\n",
    "    chain[sample, 0] = mean_sample\n",
    "    chain[sample, 1] = np.sqrt(variance_sample)\n",
    "\n",
    "    # update mcmc sampler's priors with samples of the prior parameters,\n",
    "    for i, (log_posterior, sampler) in enumerate(zip(log_posteriors, samplers)):\n",
    "        log_posterior._log_prior._priors[1].__init__(mean_sample,np.sqrt(variance_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Samples of hyper-parameters $k$ and $\\sigma_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pints.plot\n",
    "ks = [real[1] for real in real_parameters]\n",
    "pints.plot.trace([chain],ref_parameters=[np.mean(ks),np.std(ks)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Samples of parameters $k_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = pints.plot.trace(k_chains)\n",
    "axes[0, 0].get_legend().remove()\n",
    "fig.set_size_inches(10,6)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Gibbs sampling is a way of breaking up a high-dimensional problem into more managable sub-problems by conditioning on the other parameters\n",
    "- Hierachical problems arise by considering generating models for parameters\n",
    "- Sampling from hierachical models is often problem specific, but can (in certain cases) be automated (see probabilistic programming)\n",
    "- PINTS can sample from hiearachical models using the ask-and-tell interface"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
