{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider, IntSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional distributions\n",
    "\n",
    "### Example Two-dimensional distribution:\n",
    "- Imagine you are interested in the interrelation between the circumference of a person’s head ($H$) and the volume of their brain ($B$).\n",
    "- Based on data we find there is a positive correlation between these two variables, which we represent in a distribution $P(H, B)$.\n",
    "\n",
    "![](fig/Human-brain.SVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def brain_pdf(xy):\n",
    "    return multivariate_normal.pdf(xy, mean=[45, 1500], cov=[[10**2, 5*100],[5*100,200**2]])\n",
    "\n",
    "def get_full_brain_pdf():\n",
    "    x = np.linspace(20, 70, 50)\n",
    "    y = np.linspace(1000, 2000, 40)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XY = np.column_stack([X.flat, Y.flat])\n",
    "\n",
    "    Z = brain_pdf(XY).reshape(X.shape)\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "def get_conditional_brain_pdf():\n",
    "    x = np.linspace(20, 70, 50)\n",
    "    y = 1450*np.ones_like(x)\n",
    "    \n",
    "    xy = np.column_stack([x, y])\n",
    "    z = brain_pdf(xy).reshape(x.shape)\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def show_brain_distribution(conditional=False):\n",
    "    X, Y, Z = get_full_brain_pdf()\n",
    "    plt.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    \n",
    "    if conditional:\n",
    "        plt.plot([20,70],[1450, 1450])\n",
    "    plt.xlabel(r'$H$')\n",
    "    plt.ylabel(r'$B$')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def show_brain_distribution_conditional():   \n",
    "    X, Y, Z = get_full_brain_pdf()\n",
    "    x, y, z = get_conditional_brain_pdf()\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "\n",
    "    ax1.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    ax1.plot([20,70],[1450, 1450])\n",
    "    ax2.plot(x, z)\n",
    "    ax2.fill_between(x,z,alpha=0.2)\n",
    "    ax1.set_xlabel(r'$H$')\n",
    "    ax1.set_ylabel(r'$B$')\n",
    "    ax2.set_ylabel(r'$P(B = 1450, H)$')\n",
    "    ax2.set_xlabel(r'$H$')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_brain_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional distributions\n",
    "\n",
    "- **Question:** If an individual has a brain volume of 1450cm3, then what does the distribution for their head circumference look like?\n",
    "- **Answer:** Use law of conditional probability:\n",
    "\n",
    "$$P(H|B = 1450) = \\frac{P(B = 1450, H)} {P(B = 1450)}$$\n",
    "\n",
    "- **Analogy:** imagine walking over the probability distribution along a line of B = 1450cm3, and recording your height as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_brain_distribution_conditional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs sampler\n",
    "\n",
    "- A dependent sampling technique\n",
    "- Useful for hierarchical models: breaks up higher-dimensional problems into separate lower-dimensional problems, *conditioned* on the remaining variables.\n",
    "- Can be used in conjunction with Random Walk Metropolis method $\\Rightarrow$ still allows us to use all the machinery we've already developed!\n",
    "- Named after Josiah Willard Gibbs (February 11, 1839 – April 28, 1903), algorithm proposed by Stuart and Donald Geman in 1984\n",
    "\n",
    "![](fig/Josiah_Willard_Gibbs_-from_MMS-.jpg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Gibbs sampler\n",
    "\n",
    "For a parameter vector: $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$:\n",
    "\n",
    "- Select a random starting location: $(\\theta_1^0, \\theta_2^0, \\theta_3^0)$, along the same lines as for Random Walk Metropolis.\n",
    "- For each iteration $t = 1, ..., T$ do:\n",
    "    1. Select a random parameter update ordering, for example $(\\theta_3, \\theta_2, \\theta_1)$.\n",
    "    2. Independently sample from the conditional posterior for each parameter in order using the most up-to-date parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Gibbs sampler\n",
    "\n",
    "First we sample:\n",
    "    \n",
    "$$\\theta^1_3 \\sim P(\\theta_3|\\theta^0_2, \\theta_1^0)$$\n",
    "\n",
    "Then conditional on freshly-sampled $\\theta^1_3$\n",
    "\n",
    "$$\\theta^1_2 \\sim P(\\theta_2|\\theta^1_3, \\theta_1^0)$$\n",
    "\n",
    "Then conditional on freshly-sampled $\\theta^1_3$ and $\\theta_2^1$:\n",
    "\n",
    "$$\\theta^1_1 \\sim P(\\theta_1|\\theta^1_3, \\theta_2^1)$$\n",
    "\n",
    "**Important:** in Gibbs sampling there is no rejection of steps \n",
    "\n",
    "$\\Rightarrow$ unlike Random Walk Metropolis!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example application of Gibbs sampling: : speed of motion of neighbouring birds in a flock\n",
    "\n",
    "Suppose we record the speed of bird A ($v_A$) and bird B ($v_B$) in\n",
    "a flock along a particular axis.\n",
    "\n",
    "Based on observations we find that the joint posterior\n",
    "distribution over speeds is a multivariate normal distribution:\n",
    "\n",
    "\n",
    "$$\\begin{pmatrix} v_A \\\\ v_B \\end{pmatrix} \\sim N \\left [ \\begin{pmatrix} v_0 \\\\ v_0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right ]$$\n",
    "\n",
    "Of course here we have an analytic expression for the posterior distribution, but this example illustrates how the method works for more general problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bird_pdf(xy):\n",
    "    return multivariate_normal.pdf(xy, mean=[0, 0], cov=[[1, 0.5],[0.5,1]])\n",
    "\n",
    "def get_full_bird_pdf():\n",
    "    x = np.linspace(-5, 5, 50)\n",
    "    y = np.linspace(-5,5, 40)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XY = np.column_stack([X.flat, Y.flat])\n",
    "\n",
    "    Z = bird_pdf(XY).reshape(X.shape)\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "def show_bird_distribution():\n",
    "    X, Y, Z = get_full_bird_pdf()\n",
    "    plt.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    plt.xticks([0],['v_0'])\n",
    "    plt.yticks([0],['v_0'])\n",
    "    plt.xlabel('speed of bird A')\n",
    "    plt.ylabel('speed of bird B')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_bird_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the conditional distributions\n",
    "\n",
    "- In many circumstances we cannot find the conditional distributions however here it is possible.\n",
    "\n",
    "- If we knew $v_B$ :\n",
    "\n",
    "$$v_A \\sim N(v_0 + \\rho(v_B − v_0), 1 − \\rho^2)$$\n",
    "\n",
    "- Alternatively, if we knew $v_A$:\n",
    "\n",
    "$$v_B \\sim N(v_0 + \\rho(v_A − v_0), 1 − \\rho^2)$$\n",
    "\n",
    "Use Gibbs sampling to conditionally sample: $v_A|v_B$ then $v_B |v_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gibbs_sample(n):\n",
    "    v0 = 0\n",
    "    rho = 0.5\n",
    "    samples = np.empty((2,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        # sample from v_A\n",
    "        samples[0,i] = np.random.normal(v0 + rho * (samples[1,i-1] - v0), 1 - rho**2)\n",
    "        \n",
    "        # sample from v_B\n",
    "        samples[1,i] = np.random.normal(v0 + rho * (samples[0,i] - v0), 1 - rho**2)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gibbs_sampling(n):\n",
    "    samples = gibbs_sample(n)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    X, Y, Z = get_full_bird_pdf()\n",
    "    ax1.contourf(X, Y, Z, 20, cmap='RdGy');\n",
    "    ax1.scatter(samples[0,:],samples[1,:],alpha=0.6)\n",
    "    ax1.set_xticks([0],['v_0'])\n",
    "    ax1.set_yticks([0],['v_0'])\n",
    "    ax1.set_xlabel('speed of bird A')\n",
    "    ax1.set_ylabel('speed of bird B')\n",
    "    \n",
    "    ax2.hist2d(samples[0,:],samples[1,:],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_gibbs_sampling, n= IntSlider(value=10, min=0, max=6000, step=100, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "model = pints.toy.LogisticModel()\n",
    "\n",
    "n = 5\n",
    "noise = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_hyper_parameters = [500, 50]\n",
    "real_parameters = [\n",
    "    np.array([0.015, np.random.normal(real_hyper_parameters[0], real_hyper_parameters[1])]) \n",
    "        for i in range(n)\n",
    "]\n",
    "times = np.linspace(0, 800, 500)\n",
    "values = [\n",
    "    model.simulate(real, times) + np.random.normal(0, noise, times.shape)\n",
    "        for real in real_parameters\n",
    "]\n",
    "\n",
    "problems = [\n",
    "    pints.SingleOutputProblem(model, times, v)\n",
    "        for v in values\n",
    "]\n",
    "\n",
    "# Create a log-likelihood function\n",
    "log_likelihoods = [\n",
    "    pints.GaussianKnownSigmaLogLikelihood(problem, noise)\n",
    "        for problem in problems\n",
    "]\n",
    "\n",
    "log_prior = pints.ComposedLogPrior(\n",
    "    pints.UniformLogPrior([0.01],[0.02]),\n",
    "    pints.GaussianLogPrior(real_hyper_parameters[0],1e9*real_hyper_parameters[0])\n",
    ")\n",
    "\n",
    "\n",
    "log_posteriors = [\n",
    "    pints.LogPosterior(log_likelihood, log_prior)\n",
    "        for log_likelihood in log_likelihoods\n",
    "]\n",
    "\n",
    "\n",
    "# Choose starting points for k mcmc chains\n",
    "starting_points = [\n",
    "    [ real ]\n",
    "    for real in real_parameters\n",
    "]\n",
    "\n",
    "\n",
    "samplers = [\n",
    "    pints.AdaptiveCovarianceMCMC(x0)\n",
    "     for x0 in starting_points\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.stats\n",
    "\n",
    "# prior hyperparameters\n",
    "sigma0 = 100\n",
    "k_0 = 0\n",
    "\n",
    "mu_0 = real_hyper_parameters[0]\n",
    "alpha_0 = 1\n",
    "beta_0 = 1.0e-9 * sigma0\n",
    "\n",
    "samples = 20000\n",
    "chain = np.empty((samples, 2))\n",
    "k_chains = [np.empty((samples, 2)) for i in range(n)]\n",
    "\n",
    "for sample in range(samples):\n",
    "    if sample % 10 == 0:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # generate samples of lower level samplers\n",
    "    xs = np.empty((2, k))\n",
    "    error = 0\n",
    "    for i, (sampler, log_posterior) in enumerate(zip(samplers, log_posteriors)):\n",
    "        x = sampler.ask()\n",
    "        xs[:,i] = sampler.tell(log_posterior(x))\n",
    "        k_chains[i][sample, :] = xs[:, i]\n",
    "    \n",
    "    # sample mean and covariance from a normal inverse wishart\n",
    "    xhat = np.mean(xs[1,:])\n",
    "    var = np.sum((xs[1,:]-xhat)**2)\n",
    "\n",
    "    k = k_0 + n\n",
    "    mu = (k_0 * mu_0 + n * xhat) / k\n",
    "    alpha = alpha_0 + n/2\n",
    "    beta = beta_0 + 0.5*(var + (k_0 * n) / k * (xhat - mu_0)**2)\n",
    "    \n",
    "    variance_sample = scipy.stats.invgamma.rvs(a=alpha, loc=0, scale=beta)\n",
    "    mean_sample = scipy.stats.norm.rvs(mu,variance_sample / k)\n",
    "\n",
    "    # store sample to chain\n",
    "    chain[sample, 0] = mean_sample\n",
    "    chain[sample, 1] = np.sqrt(variance_sample)\n",
    "\n",
    "    # replace individual sampler's priors with hierarchical params,\n",
    "    for i, (log_posterior, sampler) in enumerate(zip(log_posteriors, samplers)):\n",
    "        log_posterior._log_prior._priors[1].__init__(mean_sample,np.sqrt(variance_sample))\n",
    "\n",
    "import pints.plot\n",
    "pints.plot.trace([chain])\n",
    "pints.plot.trace([k_chains[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
