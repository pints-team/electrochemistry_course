{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "- Recall the likelihood appears in Bayes' Theorem \n",
    "\n",
    "$$P(\\theta | data) = \\frac{\\color{red}{{P(data|\\theta)}} P(\\theta)}{P(data)}$$\n",
    "\n",
    "- For any given $\\theta$, this gives a valid probability distribution\n",
    "- If $\\theta$ is unknown, then this does *not* give a valid probability distribution (example? coin?)\n",
    "\n",
    "- For variable $\\theta$, we therefore tend to introduce the term *likelihood* to describe $P(data|\\theta)$\n",
    "\n",
    "- A notation often seen in the literature is\n",
    "\n",
    "$$\\mathcal{L}(\\theta | data) = P(data | \\theta)$$\n",
    "\n",
    "Therefore, a likelihood of $\\theta$ for a particular data sample is equivalent to the probability of that data sample for that value of $\\theta$. We call the above the *equivalence relation*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing an approprate likelihood\n",
    "\n",
    "- Likelihood is based on your model. This could be a purely statistical model, for example our coin toss example:\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- Back to the coin example. We perform an experiment of $n$ flips, and it lands heads up $h$ times\n",
    "\n",
    "- Likelihood is:\n",
    "\n",
    "$$P(h\\times H | \\theta) = \\theta^h (1-\\theta)^{n-h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: disease prevalence of a group\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: ODE-based model\n",
    "\n",
    "- In science, these models often take the form of differential equations\n",
    "- Lets consider the case of an ordinary differential equation (ODE) model with a time-dependent solution\n",
    "\n",
    "$$$$\n",
    "Science has a long tradition of modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "- In the previous lecture we worked out the posterior *distribution* a given parameter, given the available data\n",
    "- Often only interested in the most *likely* parameter value\n",
    "- Can use maximum likelihood estimation, simply find the value of \\theta that maximises the likelihood\n",
    "- a **frequentist** approach, uses a likelihood function (not a valid probability distribution)\n",
    "\n",
    "$$\\theta_{mle} = \\text{arg max}_{\\theta \\in \\Omega} P(data|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Coin example\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- Back to the coin example. We perform an experiment of $n$ flips, and it lands heads up $h$ times\n",
    "\n",
    "- Likelihood is:\n",
    "\n",
    "$$P(h\\times H | \\theta) = \\theta^h (1-\\theta)^{n-h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin example\n",
    "\n",
    "calculate derivative, set to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example using PINTS\n",
    "\n",
    "- We will use a popular model of population growth, the logistic equation:\n",
    "\n",
    "    $$ \\frac{df(t)}{dt} = r f(t) \\frac{k - f(t)}{k}$$\n",
    "    $$f(t) = \\frac{k}{1+(k/p_0 - 1) \\exp(-r t)}$$\n",
    "    \n",
    "- Two parameters, the carrying capacity $k$, and the rate of growth $r$\n",
    "- The `pints.GaussianLogLikelihood` in PINTS implements the independent Gaussian noise log-likelihood derived earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "p0 = 1  # initial population; initial value\n",
    "model = pints.toy.LogisticModel(p0)\n",
    "\n",
    "# Define the 'true' parameters\n",
    "true_parameters = [0.1, 50, 5]\n",
    "\n",
    "# Run a simulation to get test data\n",
    "times = np.linspace(0, 100, 100)\n",
    "values = model.simulate(true_parameters[:-1], times)\n",
    "\n",
    "# Add some noise\n",
    "values += np.random.normal(0, true_parameters[-1], values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the test data\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "plt.plot(times, model.simulate(true_parameters[:-1], times), ls='--')\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Create the log-likelihood function\n",
    "log_likelihood = pints.GaussianLogLikelihood(problem)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0, 0, 0], [100, 100, 100])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [50, 50, 50]\n",
    "\n",
    "# Perform an optimization using XNES. \n",
    "found_parameters, found_value = pints.optimise(log_likelihood, x0, boundaries=boundaries, method=pints.XNES)\n",
    "print('log_likelihood at true solution:')\n",
    "print(log_likelihood(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "found_mean = model.simulate(found_parameters[:-1], times)\n",
    "plt.fill_between(times, found_mean - found_parameters[-1], found_mean + found_parameters[-1],\n",
    "                 color='gray', alpha=0.2)\n",
    "plt.plot(times, found_mean)\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum a posteriori estimation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin example, gaussian prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic growth example\n",
    "\n",
    "Recall the logistic equation:\n",
    "    \n",
    "$$f(t) = \\frac{k}{1+(k/p_0 - 1) \\exp(-r t)}$$\n",
    "\n",
    "- Anyone familiar with this equation could estimate a value of the carrying capacity $k$ from a plot\n",
    "- Would be reasonable to therefore use a Gaussian Prior for $k$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "plt.plot(times, values)\n",
    "plt.plot([0, 100], [50, 50], c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the log-likelihood function\n",
    "log_likelihood = pints.GaussianLogLikelihood(problem)\n",
    "\n",
    "# Create a uniform prior over r\n",
    "log_prior_r = pints.UniformLogPrior([0],[100])\n",
    "\n",
    "# Create a gaussian prior over k\n",
    "log_prior_k = pints.GaussianLogPrior(50,10)\n",
    "\n",
    "# Create a uniform prior over sigma\n",
    "log_prior_sigma = pints.UniformLogPrior([0],[100])\n",
    "\n",
    "# Create a composed prior\n",
    "log_prior = pints.ComposedLogPrior(log_prior_r, log_prior_k, log_prior_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a posterior log-likelihood (log(likelihood * prior))\n",
    "log_posterior = pints.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0, 0, 0], [100, 100, 100])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [50, 50, 50]\n",
    "\n",
    "# Perform an optimization using Particle Swarm Optimisation (PSO). \n",
    "found_parameters, found_value = pints.optimise(log_likelihood, x0, boundaries=boundaries, method=pints.PSO)\n",
    "print('posterior log-likelihood at true solution:')\n",
    "print(log_posterior(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "found_mean = model.simulate(found_parameters[:-1], times)\n",
    "plt.fill_between(times, found_mean - found_parameters[-1], found_mean + found_parameters[-1],\n",
    "                 color='gray', alpha=0.2)\n",
    "plt.plot(times, found_mean)\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Electrochemistry example - POMS\n",
    "\n",
    "- three unresolved two-electron surface-confined polyoxometalate reduction processes by AC voltammetry\n",
    "\n",
    "![](fig/pom.svg)\n",
    "**(left)** Molecular structure of $[\\text{PMo}_{12}\\text{O}_{40}]^{3-}$    **(right)** Experimental AC voltammetry trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The sequence of six electron transfer steps are modelled by the following quasi-reversible reactions\n",
    "\n",
    "\\begin{align}\n",
    "    A + e^- \\underset{k^1_{ox}(t)}{\\overset{k^1_{red}(t)}{\\rightleftarrows}} B,\n",
    "    \\\\\n",
    "    B + e^- \\underset{k^2_{ox}(t)}{\\overset{k^2_{red}(t)}{\\rightleftarrows}} C,\n",
    "    \\\\\n",
    "    C + e^- \\underset{k^3_{ox}(t)}{\\overset{k^3_{red}(t)}{\\rightleftarrows}} D,\n",
    "    \\\\\n",
    "    D + e^- \\underset{k^4_{ox}(t)}{\\overset{k^4_{red}(t)}{\\rightleftarrows}} E,\n",
    "    \\\\\n",
    "    E + e^- \\underset{k^5_{ox}(t)}{\\overset{k^5_{red}(t)}{\\rightleftarrows}} F,\n",
    "    \\\\\n",
    "    F + e^-  \\underset{k^6_{ox}(t)}{\\overset{k^6_{red}(t)}{\\rightleftarrows}} G,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where the forward $k_{red}$ and backwards $k_{ox}$ reaction rates are\n",
    "given by\n",
    "the Butler-Volmer\n",
    "relationships\n",
    "\n",
    "\\begin{align}\\label{eq:rate1}\n",
    "    k^i_{red}(t) &= k^0_i \\exp\\left(-\\frac{\\alpha_i F}{RT} [E_r(t) - E^0_i]\n",
    "    \\right), \\\\\n",
    "    k^i_{ox}(t) &= k^0_i \\exp\\left((1-\\alpha_i)\\frac{F}{RT} [E_r(t) - E^0_i]\n",
    "\\right).  \\label{eq:rate2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This can be modelled by an ordinary differential equation containing 17 parameters to be estimated\n",
    "\n",
    "$$\n",
    "\\mathbf{p} =\n",
    "(E^0_1,E^0_2,E^0_3,E^0_4,E^0_5,E^0_6,k^0_1,k^0_2,k^0_3,k^0_4,k^0_5,k^0_6,\n",
    "         \\alpha_1,\n",
    "         \\alpha_2,\n",
    "         R_u,\n",
    "         C_{dl},\n",
    "         \\Gamma).\n",
    "$$\n",
    "\n",
    "- The effect of the $E^0_i$ parameters on the simulated current is highly non-linear.\n",
    "- In such a high dimensional space all non-linear optimisers we tried failed to find the global minimum\n",
    "- But approximate values of $E^0_i$ can be easily read off the experimental current trace.... **solution:** put a Gaussian prior on all $E^0_i$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**standard deviation** of the Gaussian prior (i.e. confidence of the estimation of $E^0_i$), required to be $<= 0.1$ V for **reliable parameter estimation**\n",
    "\n",
    "![](fig/quasireversible.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Revisiting the independent noise assumption\n",
    "\n",
    "![](fig/danger.png)\n",
    "\n",
    "- Assuming independent, Gaussian measurement noise results in the following log-likelihood:\n",
    "\n",
    "- Independent noise is easy to use, and often makes intuative sense.\n",
    "- **Check that this assumption is valid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
