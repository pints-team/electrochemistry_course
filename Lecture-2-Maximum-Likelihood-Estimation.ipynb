{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##################################################\n",
    "##### Matplotlib boilerplate for consistency #####\n",
    "##################################################\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import FloatSlider\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "global_fig_width = 8\n",
    "global_fig_height = global_fig_width / 1.61803399\n",
    "font_size = 12\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.edgecolor'] = '0.8'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelpad'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.titlepad'] = 16.0\n",
    "plt.rcParams['axes.titlesize'] = font_size * 1.4\n",
    "plt.rcParams['figure.figsize'] = (global_fig_width, global_fig_height)\n",
    "plt.rcParams['font.sans-serif'] = ['Computer Modern Sans Serif', 'DejaVu Sans', 'sans-serif']\n",
    "plt.rcParams['font.size'] = font_size\n",
    "plt.rcParams['grid.color'] = '0.8'\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth'] = 2\n",
    "plt.rcParams['lines.dash_capstyle'] = 'round'\n",
    "plt.rcParams['lines.dashed_pattern'] = [1, 4]\n",
    "plt.rcParams['xtick.labelsize'] = font_size\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.labelsize'] = font_size\n",
    "plt.rcParams['ytick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Likelihood\n",
    "\n",
    "- Recall the likelihood appears in Bayes' Theorem \n",
    "\n",
    "$$P(\\theta | data) = \\frac{\\color{red}{{P(data|\\theta)}} P(\\theta)}{P(data)}$$\n",
    "\n",
    "- Remember not a probability distribution because $\\theta$ varies\n",
    "- Most important choice, derived from the statistical model of the underlying process\n",
    "- Encapsulates many subjective judgements about analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Equivalence relation\n",
    "\n",
    "- A notation often seen in the literature is\n",
    "\n",
    "$$\\mathcal{L}(\\theta | data) = P(data | \\theta)$$\n",
    "\n",
    "Therefore, a likelihood of $\\theta$ for a particular data sample is equivalent to the probability of that data sample for that value of $\\theta$. We call the above an *equivalence relation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example:  frequency of lift malfunctioning\n",
    "\n",
    "\n",
    "- Imagine we want to create a model for the frequency a lift (elevator) breaks down in a given year, $X$.\n",
    "- Assume a range of unpredictable and uncorrelated factors (temperature, lift usage, etc.) affect the functioning of the lift.\n",
    "- therefore, $X ∼ \\text{Poisson}(\\theta)$, where $\\theta$ is the mean number of times the lift breaks in one year.\n",
    "- we don’t a priori know the true value of $\\theta$, our model defines collection of probability models; one for each value of $\\theta$.\n",
    "- We call this collection of models the Likelihood.\n",
    "\n",
    "![](fig/miscellaneous-elevator_repairman-elevators-lifts-repairs-repairmen-mban2852_low.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import poisson\n",
    "def lift_likelihood(theta, data):\n",
    "    x = np.arange(0, 20)\n",
    "    plt.plot(x, poisson.pmf(x, theta), 'bo', ms=8, label='poisson pmf')\n",
    "    plt.vlines(x, 0, poisson.pmf(x, theta), colors='b', lw=5, alpha=0.5)\n",
    "    print(data)\n",
    "    if data is not None:\n",
    "        plt.vlines([data], 0, poisson.pmf(data, theta), colors='r', lw=5, alpha=0.9)\n",
    "    plt.ylim(0,0.25)\n",
    "    plt.ylabel(r'$P(X | \\theta=%s)$'%str(theta))\n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "def lift_likelihood_no_data(theta):\n",
    "    lift_likelihood(theta,None)\n",
    "def lift_likelihood_five_data(theta):\n",
    "    lift_likelihood(theta,5)\n",
    "    \n",
    "def lift_likelihood_w_theta(k):\n",
    "    theta = np.linspace(0, 20, 100)\n",
    "    plt.plot(theta, np.exp(-theta)*theta**k/(math.factorial(k)))\n",
    "    plt.ylabel(r'$P(k=%s | \\theta)$'%str(k))\n",
    "    plt.xlabel(r'mean number of breakdowns $\\theta$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Display our collection of models\n",
    "widget = FloatSlider(value=5.0, min=0.0, max=15.0, step=1.0, continuous_update=False)\n",
    "interact(lift_likelihood_no_data, theta=widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To calculate the likelihood:\n",
    "- fix the data (say number of breakdowns is measured at 5)\n",
    "- find the corresponding probability for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "interact(lift_likelihood_five_data, theta=widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the equation for a Poisson distribution, and $k=5$ as the number of breakdowns, the likelihood function is:\n",
    "    \n",
    "$$\\mathcal{L}(\\theta | k) = P(k | \\theta) = \\exp(-\\theta)\\frac{\\theta^k}{k!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lift_likelihood_w_theta(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: ODE-based model\n",
    "\n",
    "- Models can often take the form of differential equations evolving in time\n",
    "- Likelihood is often straightforward to derive and relativly cheap to calculate due to *independent* (in time) measurement noise \n",
    "- Lets consider the case of an ordinary differential equation (ODE) model, the reversible reaction model in the previous lecture:\n",
    "\n",
    "$$\\dot{y}(t) = k_1 (1 - y) - k_2 y,$$\n",
    "\n",
    "where $k_1$ represents a forward reaction rate, $k_2$ is the backward reaction rate, and $y$ represents the concentration of a chemical solute.\n",
    "\n",
    "- In an experiment, we take $N$ measurements of the system $z_i$, with $i = 0...N-1$, that are modelled with independent Gaussian measurement noise:\n",
    "\n",
    "$$z_i \\sim y(t) + N(0, \\sigma)$$\n",
    "\n",
    "- Assuming that $\\sigma$ is unknown, we now have **three** model parameters: $\\boldsymbol{\\theta} = [k_1, k_2, \\sigma]$, rather than one in the previous lift example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets look at a random realisations of this experiment, with $N=50$, $k_1=5$, $k_2=3$, and $\\sigma=0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "def r(y, t, p):\n",
    "    k1 = p[0] \n",
    "    k2 = p[1] \n",
    "    dydt = k1 * (1 - y) - k2 * y\n",
    "    return dydt\n",
    "\n",
    "def random(p):\n",
    "    y0 = 0.1 \n",
    "    times = np.linspace(0, 1, 50)\n",
    "    values = odeint(r, y0, times, (p,))\n",
    "    values += np.random.normal(0, 0.02, values.shape)\n",
    "    plt.ylabel('Concentration')\n",
    "    plt.scatter(times, values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random([5,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As before, this model describes an infinite family of probability distributions governed by the three parameters $\\boldsymbol{\\theta} = [k_1, k_2, \\sigma]$\n",
    "- However, now we have $N$ outputs due to the $N$ time samples that were measured, therefore a probabilty distribution around each time point $t_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import norm\n",
    "def family_of_models(k1, k2, sigma):\n",
    "    y0 = 0.1 \n",
    "    times = np.linspace(0, 1, 50)\n",
    "    index = int(len(times)/2)\n",
    "    values = odeint(r, y0, times, ([k1, k2],)).reshape(-1)\n",
    "    global_fig_width = 8\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8/1.618))\n",
    "    ax1.set_ylabel('Concentration')    \n",
    "    ax1.set_xlabel(r'$t$')\n",
    "    ax1.plot(times, values)\n",
    "    ax1.fill_between(times, values-sigma, values+sigma, alpha=0.5)\n",
    "    ax1.axvline(times[index],color='r')\n",
    "    \n",
    "    concentrations = np.linspace(0, 1, 100)\n",
    "    ax2.plot(concentrations, norm.pdf(concentrations, values[index], sigma), color='r')\n",
    "    ax2.set_xlabel('concentration')\n",
    "    ax2.set_ylabel(r'$P(y(t_i) | \\theta)$')\n",
    "    plt.show()\n",
    "\n",
    "k1_widget = FloatSlider(value=5.0, min=0.0, max=15.0, step=1.0, continuous_update=False)\n",
    "k2_widget = FloatSlider(value=3.0, min=0.0, max=15.0, step=1.0, continuous_update=False)\n",
    "sigma_widget = FloatSlider(value=0.1, min=0.0, max=0.2, step=0.01, continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Display our collection of models\n",
    "interact(family_of_models, k1=k1_widget, k2=k2_widget, sigma=sigma_widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Likelihood of ODE model\n",
    "\n",
    "- we assume that the errors at each time point are independent, therefore the conditional probability density of observing the whole experimental trace from time sample 1 to time sample N is simply the product of the probability density functions at each time point\n",
    "\n",
    "$$\\mathcal{L}(\\theta \\vert \\mathbf{y}) = \\prod_{i=1}^{N} P(z_i | \\boldsymbol{\\theta}).$$\n",
    "\n",
    "- With our further assumption that the experimental noise is also normally distributed with a mean of zero and variance of $\\sigma^2$ , the likelihood can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\boldsymbol{\\theta} \\vert \\mathbf{z}) =  \\prod_{i=1}^{N} \\mathcal{N}(z_i \\vert y(t, \\boldsymbol{\\theta}),\\sigma^2) = \\prod_{i=0}^{N} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{\\left(z_i -y(t_i, \\boldsymbol{\\theta})\\right)^2}{2\\sigma^2}\\right),\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Normally work with the log-likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "l(\\boldsymbol{\\theta} \\vert \\mathbf{z}) &= \\log(L(\\boldsymbol{\\theta} \\vert \\mathbf{z})) \\nonumber \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (z_i - y(t_i, \\boldsymbol{\\theta}))^2 \\nonumber \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi) - N\\log(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (z_i - y (t_i, \\boldsymbol{\\theta}))^2\n",
    "\\end{align}\n",
    "\n",
    "- Notice the similarities with classical sum of squares error function (for fixed $\\sigma$ anyway)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "- In the previous lecture we worked out the posterior *distribution* a given parameter, given the available data\n",
    "- Often only interested in the most *likely* parameter value\n",
    "- Can use maximum likelihood estimation, simply find the value of \\theta that maximises the likelihood\n",
    "- a **frequentist** approach, uses a likelihood function (not a valid probability distribution)\n",
    "\n",
    "$$\\theta_{mle} = \\text{arg max}_{\\theta \\in \\Omega} P(data|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Coin example\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- Back to the coin example. We perform an experiment of $n$ flips, and it lands heads up $h$ times\n",
    "\n",
    "- Likelihood is:\n",
    "\n",
    "$$L(\\theta \\vert h\\times H) = \\theta^h (1-\\theta)^{n-h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding the parameters that maximise the Likelihood\n",
    "\n",
    "- For tractable likelihood functions, we can simply find the derivative of the likelihood and set to zero:\n",
    "    \n",
    "$$\\frac{\\partial L(\\theta \\vert h\\times H)}{\\partial \\theta} = \\theta^{h - 1} (h - n t) (-t + 1)^{-h + n - 1}$$\n",
    "\n",
    "- Set $\\frac{\\partial L(\\theta \\vert h\\times H)}{\\partial \\theta} = 0$ to find the MLE estimator for $\\theta$\n",
    "\n",
    "$$\\theta_{mle} = \\frac{h}{n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-linear optimisation\n",
    "\n",
    "- Often the analytical approach is not feasible, can then turn to one of many non-linear optimisation algorithms\n",
    "- Derivative-free Direct methods (aka Search methods)\n",
    "  - Brute-force exploration (look at the landscape and pick the lowest point)\n",
    "  - Random search (e.g. Simulated annealing) \n",
    "  - Coordinate search (aka Coordinate descent, aka Compass search)\n",
    "  - Search & poll / Pattern search\n",
    "  - Simplex methods (e.g. Nelder-Mead)\n",
    "  - Tabu search\n",
    "  - Dividing rectangles algorithm (DIRECT)\n",
    "  - Powell's conjugate direction method\n",
    "- Multi-start methods\n",
    "  - Multi-level single-linkage (MLSL)\n",
    "  - Basin-hopping\n",
    "- Derivative-free evolutionary methods and metaheuristics\n",
    "  - Genetic algorithms (GA)\n",
    "  - Differential evolution\n",
    "  - Evolution strategies\n",
    "  - Controlled random search (CRS)\n",
    "  - Swarm algorithms\n",
    "  - Metaheuristics\n",
    "- Bayesian optimistation\n",
    "- Gradient-estimating methods\n",
    "  - Finite difference methods\n",
    "  - Simplex gradient methods / Implicit filtering\n",
    "  - Natural evolution strategies (NES)\n",
    "- Surrogate-model methods\n",
    "  - Trust-region methods\n",
    "  - Data-based Online Nonlinear Extremumseeker (DONE)\n",
    "- Methods requiring the 1st-order gradient\n",
    "  - Root finding methods (e.g. Newton's method, BFGS)\n",
    "  - Gradient descent (aka Steepest descent)\n",
    "  - Stochastic gradient descent\n",
    "  - Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some useful non-linear optimisation packages:\n",
    "\n",
    "- [NLopt](https://nlopt.readthedocs.io)\n",
    "- [Pagmo/Pygmo](http://esa.github.io/pygmo/)\n",
    "- [Scipy](https://docs.scipy.org/doc/scipy/reference/optimize.html)\n",
    "- [Pints](https://pints.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example using PINTS\n",
    "\n",
    "- We will use a popular model of population growth, the logistic equation:\n",
    "\n",
    "    $$ \\frac{df(t)}{dt} = r f(t) \\frac{k - f(t)}{k}$$\n",
    "    $$f(t) = \\frac{k}{1+(k/p_0 - 1) \\exp(-r t)}$$\n",
    "    \n",
    "- Two parameters, the carrying capacity $k$, and the rate of growth $r$\n",
    "- We will assume an unknown measurement noise $\\sigma$, which gives the parameter set $\\boldsymbol{\\theta} = [r, k, \\sigma]$\n",
    "- The `pints.GaussianLogLikelihood` in PINTS implements the independent Gaussian noise log-likelihood derived earlier\n",
    "\n",
    "\\begin{align}\n",
    "l(\\boldsymbol{\\theta} \\vert \\mathbf{z}) &= \\log(L(\\boldsymbol{\\theta} \\vert \\mathbf{z})) \\nonumber \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi) - N\\log(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (z_i - y (t_i, \\boldsymbol{\\theta}))^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "p0 = 1  # initial population; initial value\n",
    "model = pints.toy.LogisticModel(p0)\n",
    "\n",
    "# Define the 'true' parameters\n",
    "true_parameters = [0.1, 50, 5]\n",
    "\n",
    "# Run a simulation to get test data\n",
    "times = np.linspace(0, 100, 100)\n",
    "values = model.simulate(true_parameters[:-1], times)\n",
    "\n",
    "# Add some noise\n",
    "values += np.random.normal(0, true_parameters[-1], values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the test data\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "plt.scatter(times, values, label='data')\n",
    "plt.plot(times, model.simulate(true_parameters[:-1], times), color='r', lw=3, label='true parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Create the log-likelihood function\n",
    "log_likelihood = pints.GaussianLogLikelihood(problem)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0, 0, 0], [100, 100, 100])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [50, 50, 50]\n",
    "\n",
    "# Perform an optimization using XNES. \n",
    "found_parameters, found_value = pints.optimise(log_likelihood, x0, boundaries=boundaries, method=pints.XNES)\n",
    "print('log_likelihood at true solution:')\n",
    "print(log_likelihood(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "found_mean = model.simulate(found_parameters[:-1], times)\n",
    "plt.fill_between(times, found_mean - found_parameters[-1], found_mean + found_parameters[-1],\n",
    "                 color='gray', alpha=0.3)\n",
    "plt.plot(times, found_mean, color='r', label='found parameters')\n",
    "plt.scatter(times, values, alpha=0.5, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum a posteriori (MAP) estimation\n",
    "\n",
    "- Rather than a likelihood, can also maximise the unnormalised posterior:\n",
    "\n",
    "$$P(\\theta | data) \\sim P(data|\\theta) P(\\theta)$$\n",
    "\n",
    "$$\\theta_{map} = \\text{arg max}_{\\theta \\in \\Omega}  P(data|\\theta) P(\\theta)$$\n",
    "\n",
    "- MLE is a particular case of MAP, using a Uniform prior (just multiplies the likelihood by a constant)\n",
    "- This method is a useful method of incorporporating domain knowledge on the parameters\n",
    "- Related to regularisation in non-linear and linear optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Coin example with a Gaussian prior\n",
    "\n",
    "![](fig/coin.jpeg)\n",
    "\n",
    "- Reasonable assumption is that the coin is likely to be fair, lets use a Gaussian prior $\\mathcal{N}(0.5, \\sigma)$\n",
    "\n",
    "- Maximum a posteriori loss function $L(\\theta)$ is therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta) &= P(data|\\theta) P(\\theta) \\\\ \n",
    "&= \\theta^h (1-\\theta)^{n-h} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(\\theta - 0.5)^2}{2 \\sigma^2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_map_coin(h, sigma):\n",
    "    n = 10\n",
    "    theta = np.linspace(0,1,100)\n",
    "    L = theta**h * (1-theta)**(n-h) * np.exp(-(theta - 0.5)**2/(2*sigma**2)) / np.sqrt(2*np.pi*sigma**2)\n",
    "    plt.plot(theta,L, label='loss function')\n",
    "    plt.xlabel(r'Probability of landing heads $\\theta$')\n",
    "    plt.ylabel(r'MAP loss function $L(\\theta)$')\n",
    "    max_i = np.argmax(L)\n",
    "    plt.scatter([theta[max_i]],[L[max_i]], color='r', label='MAP estimate', s=100)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "h_widget = FloatSlider(value=8.0, min=0.0, max=10.0, step=1.0, continuous_update=False)\n",
    "sigma_widget = FloatSlider(value=0.3, min=0.0, max=0.3, step=0.01, continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(show_map_coin, h=h_widget, sigma=sigma_widget, continuous_update=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic growth example\n",
    "\n",
    "Recall the logistic equation:\n",
    "    \n",
    "$$f(t) = \\frac{k}{1+(k/p_0 - 1) \\exp(-r t)}$$\n",
    "\n",
    "- Anyone familiar with this equation could estimate a value of the carrying capacity $k$ from a plot\n",
    "- Would be reasonable to therefore use a Gaussian Prior for $k$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_logistic_estimate():\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(r'Population $y(t)$')\n",
    "    plt.scatter(times, values, label='data')\n",
    "    plt.plot([0, 100], [50, 50], c='k', ls='--', lw=3, label='estimate for $k$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(values)\n",
    "show_logistic_estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding MAP estimator in PINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create the log-likelihood function (using the problem defined earlier)\n",
    "log_likelihood = pints.GaussianLogLikelihood(problem)\n",
    "\n",
    "# Create a uniform prior over r\n",
    "log_prior_r = pints.UniformLogPrior([0],[100])\n",
    "\n",
    "# Create a gaussian prior over k\n",
    "log_prior_k = pints.GaussianLogPrior(50,10)\n",
    "\n",
    "# Create a uniform prior over sigma\n",
    "log_prior_sigma = pints.UniformLogPrior([0],[100])\n",
    "\n",
    "# Create a composed prior\n",
    "log_prior = pints.ComposedLogPrior(log_prior_r, log_prior_k, log_prior_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a posterior log-likelihood (log(likelihood * prior))\n",
    "log_posterior = pints.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "# Select some boundaries\n",
    "boundaries = pints.RectangularBoundaries([0, 0, 0], [100, 100, 100])\n",
    "\n",
    "# Select a starting point\n",
    "x0 = [50, 50, 50]\n",
    "\n",
    "# Perform an optimization using Particle Swarm Optimisation (PSO). \n",
    "found_parameters, found_value = pints.optimise(log_likelihood, x0, boundaries=boundaries, method=pints.PSO)\n",
    "print('posterior log-likelihood at true solution:')\n",
    "print(log_posterior(true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'Population $y(t)$')\n",
    "found_mean = model.simulate(found_parameters[:-1], times)\n",
    "plt.fill_between(times, found_mean - found_parameters[-1], found_mean + found_parameters[-1],\n",
    "                 color='gray', alpha=0.2)\n",
    "plt.plot(times, found_mean)\n",
    "plt.plot(times, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Electrochemistry example - POMS\n",
    "\n",
    "- three unresolved two-electron surface-confined polyoxometalate reduction processes by AC voltammetry\n",
    "\n",
    "![](fig/pom.svg)\n",
    "**(left)** Molecular structure of $[\\text{PMo}_{12}\\text{O}_{40}]^{3-}$    **(right)** Experimental AC voltammetry trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The sequence of six electron transfer steps are modelled by the following quasi-reversible reactions\n",
    "\n",
    "\\begin{align}\n",
    "    A + e^- \\underset{k^1_{ox}(t)}{\\overset{k^1_{red}(t)}{\\rightleftarrows}} B,\n",
    "    \\\\\n",
    "    B + e^- \\underset{k^2_{ox}(t)}{\\overset{k^2_{red}(t)}{\\rightleftarrows}} C,\n",
    "    \\\\\n",
    "    C + e^- \\underset{k^3_{ox}(t)}{\\overset{k^3_{red}(t)}{\\rightleftarrows}} D,\n",
    "    \\\\\n",
    "    D + e^- \\underset{k^4_{ox}(t)}{\\overset{k^4_{red}(t)}{\\rightleftarrows}} E,\n",
    "    \\\\\n",
    "    E + e^- \\underset{k^5_{ox}(t)}{\\overset{k^5_{red}(t)}{\\rightleftarrows}} F,\n",
    "    \\\\\n",
    "    F + e^-  \\underset{k^6_{ox}(t)}{\\overset{k^6_{red}(t)}{\\rightleftarrows}} G,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where the forward $k_{red}$ and backwards $k_{ox}$ reaction rates are\n",
    "given by\n",
    "the Butler-Volmer\n",
    "relationships\n",
    "\n",
    "\\begin{align}\\label{eq:rate1}\n",
    "    k^i_{red}(t) &= k^0_i \\exp\\left(-\\frac{\\alpha_i F}{RT} [E_r(t) - E^0_i]\n",
    "    \\right), \\\\\n",
    "    k^i_{ox}(t) &= k^0_i \\exp\\left((1-\\alpha_i)\\frac{F}{RT} [E_r(t) - E^0_i]\n",
    "\\right).  \\label{eq:rate2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This can be modelled by an ordinary differential equation containing 17 parameters to be estimated\n",
    "\n",
    "$$\n",
    "\\mathbf{p} =\n",
    "(E^0_1,E^0_2,E^0_3,E^0_4,E^0_5,E^0_6,k^0_1,k^0_2,k^0_3,k^0_4,k^0_5,k^0_6,\n",
    "         \\alpha_1,\n",
    "         \\alpha_2,\n",
    "         R_u,\n",
    "         C_{dl},\n",
    "         \\Gamma).\n",
    "$$\n",
    "\n",
    "- The effect of the $E^0_i$ parameters on the simulated current is highly non-linear.\n",
    "- In such a high dimensional space all non-linear optimisers we tried failed to find the global minimum\n",
    "- But approximate values of $E^0_i$ can be easily read off the experimental current trace.... **solution:** put a Gaussian prior on all $E^0_i$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**standard deviation** of the Gaussian prior (i.e. confidence of the estimation of $E^0_i$), required to be $<= 0.1$ V for **reliable parameter estimation**\n",
    "\n",
    "![](fig/quasireversible.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Revisiting the independent noise assumption\n",
    "\n",
    "![](fig/danger.png)\n",
    "\n",
    "- Assuming independent, Gaussian measurement noise results in the following log-likelihood:\n",
    "\n",
    "- Independent noise is easy to use, and often makes intuative sense.\n",
    "- **Check that this assumption is valid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
